{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DWAdqgS8OYp"
      },
      "source": [
        "# Reinforcement Learning — Lab 1  \n",
        "## FrozenLake (Gymnasium) + Tabular Q-learning\n",
        "\n",
        "**Goal:** get comfortable with *Gymnasium* and implement a simple *tabular Q-learning* agent on **FrozenLake-v1**.\n",
        "\n",
        "You’ll:\n",
        "1. Install and import Gymnasium\n",
        "2. Explore the environment API\n",
        "3. Implement **epsilon-greedy** action selection\n",
        "4. Implement **Q-learning**\n",
        "5. Evaluate success rate and discuss hyperparameters\n",
        "\n",
        "> Tip: If you make changes and want a clean run, use **Runtime → Restart and run all**.\n"
      ],
      "id": "4DWAdqgS8OYp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qlXS5He8OYq"
      },
      "source": [
        "### Marking / submission (suggested)\n",
        "- Your completed notebook (`.ipynb`)\n",
        "- Short answers to the questions in **Part 6**\n"
      ],
      "id": "7qlXS5He8OYq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDAQNOUe8OYr"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 1 — Setup (Google Colab)\n",
        "Run the next cell to install dependencies.\n"
      ],
      "id": "oDAQNOUe8OYr"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meI9u4_U8OYr"
      },
      "source": [
        "# Install Gymnasium (the maintained successor to OpenAI Gym).\n",
        "!pip -q install gymnasium\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "meI9u4_U8OYr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laBqRiVF8OYs"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 2 — FrozenLake basics\n",
        "\n",
        "FrozenLake is a small **discrete** environment. By default it is **slippery**, meaning actions can have stochastic outcomes.\n"
      ],
      "id": "laBqRiVF8OYs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeAEe3Eb8OYs"
      },
      "source": [
        "env = gym.make(\"FrozenLake-v1\", is_slippery=True)  # try False later\n",
        "obs, info = env.reset(seed=0)\n",
        "print(\"Initial observation:\", obs)\n",
        "print(\"Observation space:\", env.observation_space)\n",
        "print(\"Action space:\", env.action_space)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "DeAEe3Eb8OYs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSpZgYkn8OYs"
      },
      "source": [
        "### 2.1 Taking a step\n",
        "\n",
        "Gymnasium’s `step` returns: `(obs, reward, terminated, truncated, info)`\n",
        "\n",
        "- `terminated` means the episode ended due to the task outcome (goal or hole).\n",
        "- `truncated` means the episode ended due to a time limit or other constraint.\n"
      ],
      "id": "LSpZgYkn8OYs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBWdUxIX8OYs"
      },
      "source": [
        "obs, info = env.reset(seed=0)\n",
        "action = env.action_space.sample()\n",
        "next_obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "print(\"action:\", action)\n",
        "print(\"next_obs:\", next_obs)\n",
        "print(\"reward:\", reward)\n",
        "print(\"terminated:\", terminated, \"truncated:\", truncated)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "vBWdUxIX8OYs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2wyrg9X8OYt"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 3 — Epsilon-greedy policy helper\n",
        "\n",
        "Implement a function that chooses:\n",
        "- with probability `epsilon`: a **random** action\n",
        "- otherwise: the **greedy** action `argmax_a Q[s, a]`\n",
        "\n",
        "Fill in the TODOs.\n"
      ],
      "id": "k2wyrg9X8OYt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgJoAK038OYt"
      },
      "source": [
        "def epsilon_greedy_action(Q: np.ndarray, state: int, epsilon: float, n_actions: int) -> int:\n",
        "    \"\"\"Choose an action using epsilon-greedy strategy.\"\"\"\n",
        "    # TODO:\n",
        "    # - With prob epsilon, return a random action in {0,...,n_actions-1}\n",
        "    # - Otherwise return greedy action argmax_a Q[state, a]\n",
        "    #\n",
        "    # Hints:\n",
        "    #   if np.random.random() < epsilon:\n",
        "    #       return np.random.randint(n_actions)\n",
        "    #   return int(np.argmax(Q[state]))\n",
        "    raise NotImplementedError(\"TODO: implement epsilon_greedy_action\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "EgJoAK038OYt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6nU3Qm58OYt"
      },
      "source": [
        "Quick sanity check: once you implement `epsilon_greedy_action`, the cell below should run.\n",
        "\n",
        "Expected behaviour:\n",
        "- with `epsilon=1.0`, actions are basically random\n",
        "- with `epsilon=0.0`, you always take the greedy action\n"
      ],
      "id": "L6nU3Qm58OYt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckBrpTRz8OYt"
      },
      "source": [
        "# Sanity check (run after implementing epsilon_greedy_action)\n",
        "n_states = env.observation_space.n\n",
        "n_actions = env.action_space.n\n",
        "Q_test = np.zeros((n_states, n_actions))\n",
        "Q_test[0, 2] = 1.0  # make action 2 greedy in state 0\n",
        "\n",
        "for eps in [1.0, 0.2, 0.0]:\n",
        "    actions = [epsilon_greedy_action(Q_test, state=0, epsilon=eps, n_actions=n_actions) for _ in range(20)]\n",
        "    print(f\"epsilon={eps}: {actions}\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "ckBrpTRz8OYt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4tKDIXJ8OYu"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 4 — Q-learning on FrozenLake\n",
        "\n",
        "**Q-learning update:**\n",
        "\n",
        "$$\n",
        "Q[s,a] \\leftarrow Q[s,a] + \\alpha \\left(r + \\gamma \\max_{a'} Q[s',a'] - Q[s,a]\\right)\n",
        "$$\n",
        "\n",
        "We’ll track:\n",
        "- episode return (sum of rewards)\n",
        "- success rate (how often we reach the goal)\n"
      ],
      "id": "i4tKDIXJ8OYu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsKEQsKm8OYu"
      },
      "source": [
        "### 4.1 Training scaffold (fill in TODOs)\n",
        "\n",
        "Notes:\n",
        "- FrozenLake rewards are usually `0` except reaching the goal gives `1`.\n",
        "- Training can require many episodes when `is_slippery=True`.\n"
      ],
      "id": "RsKEQsKm8OYu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4_Bz25R8OYu"
      },
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class TrainStats:\n",
        "    returns: list\n",
        "    successes: list  # 1 if goal reached, else 0\n",
        "\n",
        "def train_q_learning(\n",
        "    env_id: str = \"FrozenLake-v1\",\n",
        "    episodes: int = 20000,\n",
        "    alpha: float = 0.1,\n",
        "    gamma: float = 0.99,\n",
        "    epsilon_start: float = 1.0,\n",
        "    epsilon_end: float = 0.05,\n",
        "    epsilon_decay_fraction: float = 0.8,\n",
        "    is_slippery: bool = True,\n",
        "    seed: int = 0,\n",
        "):\n",
        "    \"\"\"Train a tabular Q-learning agent on FrozenLake. Returns (Q, stats).\"\"\"\n",
        "    env = gym.make(env_id, is_slippery=is_slippery)\n",
        "    n_states = env.observation_space.n\n",
        "    n_actions = env.action_space.n\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "    Q = np.zeros((n_states, n_actions), dtype=np.float32)\n",
        "    stats = TrainStats(returns=[], successes=[])\n",
        "\n",
        "    decay_episodes = max(1, int(episodes * epsilon_decay_fraction))\n",
        "\n",
        "    def epsilon_for_episode(ep: int) -> float:\n",
        "        if ep >= decay_episodes:\n",
        "            return epsilon_end\n",
        "        # TODO: linear decay from epsilon_start to epsilon_end over [0, decay_episodes)\n",
        "        # One way:\n",
        "        #   frac = ep / decay_episodes\n",
        "        #   return epsilon_start + frac * (epsilon_end - epsilon_start)\n",
        "        raise NotImplementedError(\"TODO: implement epsilon schedule\")\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        state, info = env.reset(seed=seed + ep)\n",
        "        done = False\n",
        "        ep_return = 0.0\n",
        "\n",
        "        while not done:\n",
        "            eps = epsilon_for_episode(ep)\n",
        "            action = epsilon_greedy_action(Q, state, eps, n_actions)\n",
        "\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # TODO: Q-learning update\n",
        "            # If done: target = reward\n",
        "            # Else:    target = reward + gamma * np.max(Q[next_state])\n",
        "            # Then:\n",
        "            #   Q[state, action] += alpha * (target - Q[state, action])\n",
        "            raise NotImplementedError(\"TODO: implement Q-learning update\")\n",
        "\n",
        "            state = next_state\n",
        "            ep_return += reward\n",
        "\n",
        "        stats.returns.append(ep_return)\n",
        "        stats.successes.append(1 if ep_return > 0 else 0)\n",
        "\n",
        "    env.close()\n",
        "    return Q, stats\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "T4_Bz25R8OYu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESe1GEc18OYu"
      },
      "source": [
        "### 4.2 Run training (after TODOs are implemented)\n",
        "\n",
        "Try:\n",
        "- `episodes=20000` (slippery) then increase if needed\n",
        "- or set `is_slippery=False` to see a much easier problem\n"
      ],
      "id": "ESe1GEc18OYu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeZOz2pF8OYu"
      },
      "source": [
        "# Run training (this will work after you fill in the TODOs above)\n",
        "Q, stats = train_q_learning(\n",
        "    episodes=20000,\n",
        "    alpha=0.1,\n",
        "    gamma=0.99,\n",
        "    epsilon_start=1.0,\n",
        "    epsilon_end=0.05,\n",
        "    epsilon_decay_fraction=0.8,\n",
        "    is_slippery=True,\n",
        "    seed=0,\n",
        ")\n",
        "\n",
        "print(\"Training finished.\")\n",
        "print(\"Final 1000-episode average success rate:\", float(np.mean(stats.successes[-1000:])))\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "oeZOz2pF8OYu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDSyxfXb8OYu"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 5 — Evaluation\n",
        "\n",
        "We evaluate with `epsilon=0` (pure greedy). We'll run a number of episodes and compute:\n",
        "- average return\n",
        "- success rate\n"
      ],
      "id": "qDSyxfXb8OYu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGubbr_-8OYv"
      },
      "source": [
        "def evaluate_greedy_policy(Q: np.ndarray, env_id: str=\"FrozenLake-v1\", episodes: int=200, is_slippery: bool=True, seed: int=123):\n",
        "    env = gym.make(env_id, is_slippery=is_slippery)\n",
        "    successes = 0\n",
        "    returns = []\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        state, info = env.reset(seed=seed + ep)\n",
        "        done = False\n",
        "        ep_return = 0.0\n",
        "\n",
        "        while not done:\n",
        "            action = int(np.argmax(Q[state]))  # greedy\n",
        "            state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            ep_return += reward\n",
        "\n",
        "        returns.append(ep_return)\n",
        "        if ep_return > 0:\n",
        "            successes += 1\n",
        "\n",
        "    env.close()\n",
        "    return float(np.mean(returns)), successes / episodes\n",
        "\n",
        "avg_return, success_rate = evaluate_greedy_policy(Q, episodes=200, is_slippery=True)\n",
        "print(\"Average return:\", avg_return)\n",
        "print(\"Success rate:\", success_rate)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "sGubbr_-8OYv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTZNJ5Yv8OYv"
      },
      "source": [
        "### 5.1 Plot a learning curve\n",
        "\n",
        "The plot below shows a moving average of success rate.\n"
      ],
      "id": "NTZNJ5Yv8OYv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYBarjhT8OYv"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "window = 200\n",
        "successes = np.array(stats.successes, dtype=np.float32)\n",
        "moving = np.convolve(successes, np.ones(window)/window, mode=\"valid\")\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(moving)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(f\"Success rate (moving avg, window={window})\")\n",
        "plt.title(\"FrozenLake Q-learning training progress\")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "JYBarjhT8OYv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDhCHD8l8OYv"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 6 — Questions (write brief answers below)\n",
        "\n",
        "1. What happens to success rate when you increase vs decrease `epsilon`?\n",
        "2. What happens when you switch `is_slippery=True` to `False`?\n",
        "3. Which hyperparameter mattered most for you: `alpha`, `gamma`, the epsilon schedule, or episode count?\n"
      ],
      "id": "YDhCHD8l8OYv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaxeNMF48OYv"
      },
      "source": [
        "### Answers\n",
        "\n",
        "1.  \n",
        "2.  \n",
        "3.  \n"
      ],
      "id": "kaxeNMF48OYv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_pWmXtF8OYv"
      },
      "source": [
        "---\n",
        "\n",
        "## Optional extension — Run one greedy episode\n",
        "\n",
        "This cell runs one greedy episode and prints the visited state indices.\n"
      ],
      "id": "B_pWmXtF8OYv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29C9KIwU8OYw"
      },
      "source": [
        "def run_one_episode_greedy(Q: np.ndarray, env_id: str=\"FrozenLake-v1\", is_slippery: bool=True, seed: int=999):\n",
        "    env = gym.make(env_id, is_slippery=is_slippery)\n",
        "    state, info = env.reset(seed=seed)\n",
        "    done = False\n",
        "    states = [state]\n",
        "    total = 0.0\n",
        "\n",
        "    while not done:\n",
        "        action = int(np.argmax(Q[state]))\n",
        "        state, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        states.append(state)\n",
        "        total += reward\n",
        "\n",
        "    env.close()\n",
        "    return states, total\n",
        "\n",
        "states, total = run_one_episode_greedy(Q, is_slippery=True)\n",
        "print(\"Visited states:\", states)\n",
        "print(\"Total reward:\", total)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "29C9KIwU8OYw"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}