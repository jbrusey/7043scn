<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title>Lecture 1: Introduction to Reinforcement Learning</title>
<meta name="author" content="James Brusey"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js/dist/reveal.css"/>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js/dist/theme/serif.css" id="theme"/>

<link rel="stylesheet" href="custom.css"/>
<meta name="description" content="J Brusey - ">
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide">
<h1 class="title">Lecture 1: Introduction to Reinforcement Learning</h1><h2 class="author">James Brusey</h2>
</section>
<section>
<section id="slide-orgd5b6278">
<h2 id="orgd5b6278">Outline</h2>
<ol>
<li>Admin</li>
<li>About Reinforcement Learning</li>
<li>The Reinforcement Learning Problem</li>
<li>Inside An RL Agent</li>
<li>Problems within Reinforcement Learning</li>

</ol>
</section>
</section>
<section>
<section id="slide-orgb4aeaaf">
<h2 id="orgb4aeaaf">Admin: Textbooks</h2>
<ul>
<li><b>An Introduction to Reinforcement Learning</b>, Sutton and Barto, 2018
<ul>
<li>Available free online: <a href="http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html">http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html</a></li>

</ul></li>
<li><b>Algorithms for Reinforcement Learning</b>, Szepesvari, 2010
<ul>
<li>Available free online: <a href="http://www.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf">http://www.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf</a></li>

</ul></li>

</ul>
</section>
</section>
<section>
<section id="slide-org76c923e">
<h2 id="org76c923e">About RL</h2>
<div class="outline-text-2" id="text-org76c923e">
</div>
</section>
<section id="slide-orge2450ed">
<h3 id="orge2450ed">Many Faces of Reinforcement Learning</h3>

<div id="org3b346ac" class="figure">
<p><img src="rl-to-discipline.svg" alt="rl-to-discipline.svg" class="org-svg" height="400px" />
</p>
</div>
</section>
<section id="slide-org738ca22">
<h3 id="org738ca22">Branches of Machine Learning</h3>

<div id="org2528638" class="figure">
<p><img src="ml-types.svg" alt="ml-types.svg" class="org-svg" height="400px" />
</p>
</div>
</section>
<section id="slide-org542a95c">
<h3 id="org542a95c">Characteristics of Reinforcement Learning</h3>
<p>
What makes reinforcement learning different from other machine
learning paradigms?
</p>
<ul>
<li>No supervisor, only a <i>reward</i> signal</li>
<li>Feedback is delayed, not instantaneous</li>
<li>Time really matters (sequential, non i.i.d. data)</li>
<li>Agent‚Äôs actions affect subsequent data it receives</li>

</ul>
</section>
<section id="slide-orgecafe37">
<h3 id="orgecafe37">Examples (from 10 years ago)</h3>
<ul>
<li>Fly stunt manoeuvres in a helicopter</li>
<li>Defeat the world champion at Backgammon</li>
<li>Manage an investment portfolio</li>
<li>Control a power station</li>
<li>Make a humanoid robot walk</li>
<li>Play many different Atari games better than humans</li>

</ul>
</section>
<section id="slide-org5070505">
<h3 id="org5070505">Tesauro's TD Gammon (1992)</h3>

<div id="orged57fb3" class="figure">
<p><img src="td-gammon.png" alt="td-gammon.png" />
</p>
</div>
<aside class="notes">
<ul>
<li>first superhuman game play by RL</li>

<li>Backgammon is a game of chance but involves some decision making</li>

<li>TD Gammon is now reference for best move to make</li>

</ul>

</aside>
</section>
<section id="slide-org17b55e0">
<h3 id="org17b55e0">Fly stunt manoeuvres in a helicopter (2006)</h3>
<video controls muted data-autoplay src="Stanford Autonomous Helicopter - Chaos-kN6ifrqwIMY.mp4"> </video>
<aside class="notes">
<ul>
<li>How would you control a helicopter to perform this stunt?</li>

</ul>

</aside>
</section>
<section id="slide-orga30cf63">
<h3 id="orga30cf63">Atari DQN (Google DeepMind 2016) - Start of DeepRL</h3>
<video data-autoplay src="../2023-02-rl/figures/DQN Breakout-TmPfTpjtdgg.mp4" width="640" height="480"></video>
<aside class="notes">
<ul>
<li>prior to this - full access to internal state</li>
<li>this RL agent just sees pixel values</li>
<li>makes use of deep learning via experience replay</li>
<li>achieves superhuman performance</li>

</ul>

</aside>
</section>
<section id="slide-org872aa53">
<h3 id="org872aa53">Examples (more recently)</h3>
<ul>
<li>AlphaGo - defeat world champion at Go</li>
<li>Control cooling for a data centre</li>
<li>Find faster matrix multiplication algorithms</li>
<li>Learn to walk (robot dog) in 1 hour</li>
<li>Play StarCraft 2 better than humans</li>
<li>Drive Gran Turismo better than humans</li>
<li>Race drones faster than humans</li>
<li>Teach LLMs how to behave (RLHF)</li>

</ul>
</section>
<section id="slide-org369aafb">
<h3 id="org369aafb">Quick exercise (5 mins)</h3>
<ul>
<li>Go and find web pages, videos, or research papers for as many of the examples as you can</li>
<li>If you don't have a laptop, use your phone</li>
<li>If you don't have anything, help a neighbour</li>

</ul>
</section>
</section>
<section>
<section id="slide-org5cc5c4c">
<h2 id="org5cc5c4c">The RL Problem</h2>
<div class="outline-text-2" id="text-org5cc5c4c">
</div>
</section>
<section id="slide-orgc4d830d">
<h3 id="orgc4d830d">Reward</h3>
<ul>
<li>A reward \(R_t\) is a scalar feedback signal
<ul>
<li>Indicates how well agent is doing at step \(t\)</li>

</ul></li>
<li>The agent‚Äôs job is to maximise cumulative reward</li>
<li>Reinforcement learning is based on the <b>reward hypothesis</b></li>

</ul>
<div class="definition" id="org976708d">
<p>
All goals can be described by the maximisation of expected cumulative reward
</p>

</div>
<p>
Do you agree?
</p>
</section>
<section id="slide-orgc232a79">
<h3 id="orgc232a79">Examples of Rewards</h3>
<ul>
<li>Fly stunt manoeuvres in a helicopter
<ul>
<li>+ve reward for following desired trajectory</li>
<li>-ve reward for crashing</li>

</ul></li>
<li>Defeat the world champion at Backgammon
<ul>
<li>+/-ve reward for winning/losing a game</li>

</ul></li>
<li>Manage an investment portfolio
<ul>
<li>+ve reward for each $ in bank</li>

</ul></li>
<li>Control a power station
<ul>
<li>+ve reward for producing power</li>
<li>-ve reward for exceeding safety thresholds</li>

</ul></li>
<li>Make a humanoid robot walk
<ul>
<li>+ve reward for forward motion</li>
<li>-ve reward for falling over</li>

</ul></li>
<li>Play many different Atari games better than humans
<ul>
<li>+/-ve reward for increasing/decreasing score</li>

</ul></li>

</ul>
</section>
<section id="slide-org956f590">
<h3 id="org956f590">Sequential Decision Making</h3>
<ul>
<li>Goal: <i>select actions to maximise total future reward</i></li>
<li>Actions may have long term consequences</li>
<li>Reward may be delayed</li>
<li>It may be better to sacrifice immediate reward to gain more long-term reward</li>
<li>Examples:
<ul>
<li>A financial investment (may take months to mature)</li>
<li>Refuelling a helicopter (might prevent a crash in several hours)</li>
<li>Blocking opponent moves (might help winning chances many moves from now)</li>

</ul></li>

</ul>
</section>
<section id="slide-org214cf48">
<h3 id="org214cf48">Environments (Agent and Environment)</h3>

<div id="orga29f385" class="figure">
<p><img src="agent-env.svg" alt="agent-env.svg" class="org-svg" height="400px" />
</p>
</div>
</section>
<section id="slide-orge01a752">
<h3 id="orge01a752">Environments (Agent and Environment)</h3>
<div style="display: flex;"><div style="flex: 1; padding: 0px;">


<div id="org7a57c46" class="figure">
<p><img src="real-agent-env.svg" alt="real-agent-env.svg" class="org-svg" height="400px" />
</p>
</div>


</div><div style="flex: 1; padding: 0px; ">
<ul>
<li>At each step \(t\) the agent:
<ul>
<li>Executes action \(A_t\)</li>
<li>Receives observation \(O_t\)</li>
<li>Receives scalar reward \(R_t\)</li>

</ul></li>
<li>The environment:
<ul>
<li>Receives action \(A_t\)</li>
<li>Emits observation \(O_{t+1}\)</li>
<li>Emits scalar reward \(R_{t+1}\)</li>

</ul></li>
<li>\(t\) increments at environment step</li>

</ul>

</div></div>
</section>
<section id="slide-orgd49bc14">
<h3 id="orgd49bc14">State (History and State)</h3>
<ul>
<li>History is the sequence of observations, actions, rewards:
<ul>
<li>\(H_t = O_1, R_1, A_1, \ldots, A_{t-1}, O_t, R_t\)</li>

</ul></li>
<li>i.e. all observable variables up to time \(t\)</li>
<li>i.e. sensorimotor stream of a robot/embodied agent</li>
<li>What happens next depends on the history:
<ul>
<li>agent selects actions</li>
<li>environment selects observations/rewards</li>

</ul></li>
<li>State is the information used to determine what happens next</li>
<li>Formally, state is a function of the history:
<ul>
<li>\(S_t = f(H_t)\)</li>

</ul></li>

</ul>
</section>
<section id="slide-org0f19f49">
<h3 id="org0f19f49">State (Environment State)</h3>
<ul>
<li>The environment state \(S^e_t\) is the environment‚Äôs private representation
<ul>
<li>i.e. whatever data the environment uses to pick next observation/reward</li>

</ul></li>
<li>Environment state is not usually visible to the agent</li>
<li>Even if \(S^e_t\) is visible, it may contain irrelevant information</li>
<li>(Diagram on page 19)</li>

</ul>
</section>
<section id="slide-org544db52">
<h3 id="org544db52">State (Agent State)</h3>
<ul>
<li>The agent state \(S^a_t\) is the agent‚Äôs internal representation
<ul>
<li>i.e. whatever information the agent uses to pick the next action</li>
<li>i.e. the information used by RL algorithms</li>

</ul></li>
<li>It can be any function of history:
<ul>
<li>\(S^a_t = f(H_t)\)</li>

</ul></li>
<li>(Diagram on page 20)</li>

</ul>
</section>
<section id="slide-org864e9af">
<h3 id="org864e9af">State (Information / Markov State)</h3>
<ul>
<li>An information state (a.k.a. Markov state) contains all useful information from history.</li>

</ul>
<div class="definition" id="org056b95a">
<p>
a state \(S_t\) is Markov iff
\[
   \mathbb{P}[S_{t+1}\mid S_t] = \mathbb{P}[S_{t+1}\mid S_1,\ldots,S_t]
\]
</p>

</div>
<ul>
<li>‚ÄúThe future is independent of the past given the present‚Äù
\[
  H_{1:t} \rightarrow S_t \rightarrow H_{t+1:\infty}
  \]</li>
<li>Once the state is known, the history may be thrown away
<ul>
<li>i.e. state is a sufficient statistic of the future</li>

</ul></li>
<li>Environment state \(S^e_t\) is Markov</li>
<li>History \(H_t\) is Markov</li>

</ul>
</section>
<section id="slide-org569c126">
<h3 id="org569c126">State (Rat Example)</h3>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">üí°</td>
<td class="org-left">üí°</td>
<td class="org-left">üïπÔ∏èÔ∏è</td>
<td class="org-left">üîî</td>
<td class="org-left">&nbsp;</td>
<td class="org-left">‚ö°</td>
</tr>

<tr>
<td class="org-left">üîî</td>
<td class="org-left">üí°</td>
<td class="org-left">üïπÔ∏è</td>
<td class="org-left">üïπÔ∏è</td>
<td class="org-left">&nbsp;</td>
<td class="org-left">üßÄ</td>
</tr>

<tr>
<td class="org-left">üïπÔ∏è</td>
<td class="org-left">üí°</td>
<td class="org-left">üïπÔ∏è</td>
<td class="org-left">üîî</td>
<td class="org-left">&nbsp;</td>
<td class="org-left">?</td>
</tr>
</tbody>
</table>

<ul>
<li>What if agent state = last 3 items in sequence?</li>
<li>What if agent state = counts for lights, bells and levers?</li>
<li>What if agent state = complete sequence?</li>

</ul>
</section>
<section id="slide-orgcf49fab">
<h3 id="orgcf49fab">State (Fully Observable Environments)</h3>
<div style="display: flex;"><div style="flex: 1; padding: 0px;">


<div id="org72b3ac6" class="figure">
<p><img src="real-agent-env.svg" alt="real-agent-env.svg" class="org-svg" height="400px" />
</p>
</div>


</div><div style="flex: 1; padding: 0px; ">
<ul>
<li><b>Full observability</b>: agent <b>directly</b> observes environment state
\[
  O_t = S^a_t = S^e_t
  \]</li>
<li>Agent state = environment state = information state</li>
<li>Formally: <b>Markov decision process (MDP)</b>
<ul>
<li>(Next lecture and the majority of this course)</li>

</ul></li>

</ul>

</div></div>
</section>
<section id="slide-org1f849d8">
<h3 id="org1f849d8">State (Partially Observable Environments)</h3>
<ul>
<li><b>Partial observability</b>: agent <b>indirectly</b> observes environment:
<ul>
<li>robot with camera vision isn‚Äôt told absolute location</li>
<li>trading agent only observes current prices</li>
<li>poker-playing agent only observes public cards</li>

</ul></li>
<li>Now agent state \(\neq\) environment state</li>
<li>Formally: <b>partially observable Markov decision process</b> (POMDP)</li>
<li>Agent must construct its own state representation \(S^a_t\), e.g.
<ul>
<li>Complete history: \(S^a_t = H_t\)</li>
<li>Beliefs of environment state:
<ul>
<li>\(S^a_t = (\mathbb{P}[S^e_t=s_1], \ldots, \mathbb{P}[S^e_t=s_n])\)</li>

</ul></li>
<li>Recurrent neural network:
<ul>
<li>\(S^a_t = \sigma(S^a_{t-1} W_s + O_t W_o)\)</li>

</ul></li>

</ul></li>

</ul>
</section>
</section>
<section>
<section id="slide-org8a04cd6">
<h2 id="org8a04cd6">Inside An RL Agent</h2>
<div class="outline-text-2" id="text-org8a04cd6">
</div>
</section>
<section id="slide-org4764f35">
<h3 id="org4764f35">Major Components</h3>
<ul>
<li>An RL agent may include one or more of:
<ul>
<li>Policy: agent‚Äôs behaviour function</li>
<li>Value function: how good is each state and/or action</li>
<li>Model: agent‚Äôs representation of the environment</li>

</ul></li>

</ul>
</section>
<section id="slide-org4b625bb">
<h3 id="org4b625bb">Policy</h3>
<ul>
<li>A policy is the agent‚Äôs behaviour</li>
<li>Map from state to action, e.g.
<ul>
<li>Deterministic policy: \(a = \pi(s)\)</li>
<li>Stochastic policy: \(\pi(a\mid s)=\mathbb{P}[A_t=a\mid S_t=s]\)</li>

</ul></li>

</ul>
</section>
<section id="slide-org482ff7b">
<h3 id="org482ff7b">Value Function</h3>
<ul>
<li>Value function is a prediction of future reward</li>
<li>Used to evaluate goodness/badness of states</li>
<li>Therefore to select between actions, e.g.
<ul>
<li>\(v^\pi(s) = \mathbb{E}_\pi[ R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots \mid S_t=s]\)</li>

</ul></li>

</ul>
</section>
<section id="slide-org57138ed">
<h3 id="org57138ed">Example (Value Function in Atari)</h3>
<ul>
<li>(Image-only slide; see page 28)</li>

</ul>
</section>
<section id="slide-org4b5e351">
<h3 id="org4b5e351">Model</h3>
<ul>
<li>A model predicts what the environment will do next
<ul>
<li>\(P\) predicts next state</li>
<li>\(R\) predicts next (immediate) reward</li>

</ul></li>
<li>Example:
<ul>
<li>\(\mathcal{P}^a_{ss'} = \mathbb{P}[S_{t+1}=s' \mid S_t=s, A_t=a]\)</li>
<li>\(\mathcal{R}^a_s = \mathbb{E}[R_{t+1} \mid S_t=s, A_t=a]\)</li>

</ul></li>

</ul>
</section>
<section id="slide-org596f1fe">
<h3 id="org596f1fe">Maze Example (Start)</h3>
<ul>
<li>Goal</li>
<li>Rewards: -1 per time-step</li>
<li>Actions: N, E, S, W</li>
<li>States: agent‚Äôs location</li>
<li>(Maze diagram on page 30)</li>

</ul>
</section>
<section id="slide-org50f71e2">
<h3 id="org50f71e2">Maze Example (Policy)</h3>
<ul>
<li>Arrows represent policy \(\pi(s)\) for each state \(s\)</li>
<li>(Diagram on page 31)</li>

</ul>
</section>
<section id="slide-org009a82c">
<h3 id="org009a82c">Maze Example (Value Function)</h3>
<ul>
<li>Numbers represent value \(v_\pi(s)\) of each state \(s\)</li>
<li>(Diagram on page 32)</li>

</ul>
</section>
<section id="slide-org5180ef4">
<h3 id="org5180ef4">Maze Example (Model)</h3>
<ul>
<li>Agent may have an internal model of the environment
<ul>
<li>Dynamics: how actions change the state</li>
<li>Rewards: how much reward from each state</li>

</ul></li>
<li>The model may be imperfect</li>
<li>Grid layout represents transition model \(P^a_{ss'}\)</li>
<li>Numbers represent immediate reward \(R^a_s\) from each state \(s\) (same for all \(a\))</li>
<li>(Diagram on page 33)</li>

</ul>
</section>
<section id="slide-org1ecf826">
<h3 id="org1ecf826">Categorizing RL agents (1)</h3>
<ul>
<li>Value-Based
<ul>
<li>No policy (implicit)</li>
<li>Value function</li>

</ul></li>
<li>Policy-Based
<ul>
<li>Policy</li>
<li>No value function</li>

</ul></li>
<li>Actor-Critic
<ul>
<li>Policy</li>
<li>Value function</li>

</ul></li>

</ul>
</section>
<section id="slide-org8d58149">
<h3 id="org8d58149">Categorizing RL agents (2)</h3>
<ul>
<li>Model-Free
<ul>
<li>Policy and/or value function</li>
<li>No model</li>

</ul></li>
<li>Model-Based
<ul>
<li>Policy and/or value function</li>
<li>Model</li>

</ul></li>

</ul>
</section>
<section id="slide-orga884610">
<h3 id="orga884610">RL Agent Taxonomy</h3>
<ul>
<li>(Diagram on page 36: Model / Value Function / Policy overlap; value-based vs policy-based; model-free vs model-based; actor-critic in intersection)</li>

</ul>
</section>
</section>
<section>
<section id="slide-org9f8161c">
<h2 id="org9f8161c">Problems within RL</h2>
<div class="outline-text-2" id="text-org9f8161c">
</div>
</section>
<section id="slide-org9f0c54f">
<h3 id="org9f0c54f">Learning and Planning</h3>
<ul>
<li>Two fundamental problems in sequential decision making</li>
<li>Reinforcement Learning:
<ul>
<li>environment initially unknown</li>
<li>agent interacts with environment</li>
<li>agent improves its policy</li>

</ul></li>
<li>Planning:
<ul>
<li>model of environment is known</li>
<li>agent performs computations with model (no external interaction)</li>
<li>agent improves its policy</li>
<li>a.k.a. deliberation, reasoning, introspection, pondering, thought, search</li>

</ul></li>

</ul>
</section>
<section id="slide-org51f58d5">
<h3 id="org51f58d5">Atari Example (Reinforcement Learning)</h3>
<ul>
<li>Rules of the game are unknown</li>
<li>Learn directly from interactive gameplay</li>
<li>Pick actions on joystick, see pixels and scores</li>
<li>(Diagram on page 38)</li>

</ul>
</section>
<section id="slide-org7135004">
<h3 id="org7135004">Atari Example (Planning)</h3>
<ul>
<li>Rules of the game are known</li>
<li>Can query emulator
<ul>
<li>perfect model inside agent‚Äôs brain</li>

</ul></li>
<li>If I take action \(a\) from state \(s\):
<ul>
<li>what would the next state be?</li>
<li>what would the score be?</li>

</ul></li>
<li>Plan ahead to find optimal policy
<ul>
<li>e.g. tree search</li>

</ul></li>
<li>(Diagram on page 39)</li>

</ul>
</section>
<section id="slide-orgb0a09f5">
<h3 id="orgb0a09f5">Exploration and Exploitation (1)</h3>
<ul>
<li>RL is like trial-and-error learning</li>
<li>Agent should discover a good policy
<ul>
<li>from experiences of the environment</li>
<li>without losing too much reward along the way</li>

</ul></li>

</ul>
</section>
<section id="slide-orgd426631">
<h3 id="orgd426631">Exploration and Exploitation (2)</h3>
<ul>
<li>Exploration finds more information about the environment</li>
<li>Exploitation exploits known information to maximise reward</li>
<li>Usually important to explore as well as exploit</li>

</ul>
</section>
<section id="slide-orgc61c05b">
<h3 id="orgc61c05b">Examples (Explore/Exploit)</h3>
<ul>
<li>Restaurant selection
<ul>
<li>Exploitation: go to favourite restaurant</li>
<li>Exploration: try a new restaurant</li>

</ul></li>
<li>Online banner advertisements
<ul>
<li>Exploitation: show most successful advert</li>
<li>Exploration: show a different advert</li>

</ul></li>
<li>Oil drilling
<ul>
<li>Exploitation: drill at best known location</li>
<li>Exploration: drill at a new location</li>

</ul></li>
<li>Game playing
<ul>
<li>Exploitation: play the move believed best</li>
<li>Exploration: play an experimental move</li>

</ul></li>

</ul>
</section>
<section id="slide-org5b43003">
<h3 id="org5b43003">Prediction and Control</h3>
<ul>
<li>Prediction: evaluate the future (given a policy)</li>
<li>Control: optimise the future (find the best policy)</li>

</ul>
</section>
<section id="slide-orgc631b78">
<h3 id="orgc631b78">Gridworld Example (Prediction)</h3>
<ul>
<li>(Gridworld table + question on page 44)</li>
<li>Question: What is the value function for the uniform random policy?</li>

</ul>
</section>
<section id="slide-org843ba93">
<h3 id="org843ba93">Gridworld Example (Control)</h3>
<ul>
<li>(Figure on page 45: gridworld + \(V^*\) + \(\pi^*\))</li>
<li>Questions:
<ul>
<li>What is the optimal value function over all possible policies?</li>
<li>What is the optimal policy?</li>

</ul></li>

</ul>
</section>
</section>
<section>
<section id="slide-orged9225e">
<h2 id="orged9225e">Course Outline</h2>
<ul>
<li>Part I: Elementary Reinforcement Learning
<ol>
<li>Introduction to RL</li>
<li>Markov Decision Processes</li>
<li>Planning by Dynamic Programming</li>
<li>Model-Free Prediction</li>
<li>Model-Free Control</li>

</ol></li>
<li>Part II: Reinforcement Learning in Practice
<ol>
<li>Value Function Approximation</li>
<li>Policy Gradient Methods</li>
<li>Integrating Learning and Planning</li>
<li>Exploration and Exploitation</li>
<li>Case study ‚Äî RL in Car Climate Control</li>

</ol></li>

</ul>
</section>
</section>
</div>
</div>
<script src="https://cdn.jsdelivr.net/npm/reveal.js/dist/reveal.js"></script>
<script src="https://cdn.jsdelivr.net/npm/reveal.js/plugin/markdown/markdown.js"></script>
<script src="https://cdn.jsdelivr.net/npm/reveal.js/plugin/notes/notes.js"></script>
<script src="https://cdn.jsdelivr.net/npm/reveal.js/plugin/search/search.js"></script>
<script src="https://cdn.jsdelivr.net/npm/reveal.js/plugin/zoom/zoom.js"></script>
<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: 'c',
rollingLinks: false,
keyboard: true,
mouseWheel: false,
fragmentInURL: false,
hashOneBasedIndex: false,
pdfSeparateFragments: true,
overview: true,
margin: 0.10,
minScale: 0.10,
maxScale: 2.50,

transition: 'convex',
transitionSpeed: 'default',

// Plugins with reveal.js 4.x
plugins: [ RevealMarkdown, RevealNotes, RevealSearch, RevealZoom ],

// Optional libraries used to extend reveal.js
dependencies: [
]

});
</script>
</body>
</html>
