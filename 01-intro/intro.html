<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title>Lecture 1: Introduction to Reinforcement Learning</title>
<meta name="author" content="James Brusey"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js/dist/reveal.css"/>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js/dist/theme/serif.css" id="theme"/>

<link rel="stylesheet" href="custom.css"/>
<meta name="description" content="J Brusey - ">
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide">
<h1 class="title">Lecture 1: Introduction to Reinforcement Learning</h1><h2 class="author">James Brusey</h2>
</section>
<section>
<section id="slide-org1fd36a0">
<h2 id="org1fd36a0">Outline</h2>
<ol>
<li>Admin</li>
<li>About Reinforcement Learning</li>
<li>The Reinforcement Learning Problem</li>
<li>Inside An RL Agent</li>
<li>Problems within Reinforcement Learning</li>

</ol>
</section>
</section>
<section>
<section id="slide-org0028efd">
<h2 id="org0028efd">Admin: Textbooks</h2>
<ul>
<li><b>An Introduction to Reinforcement Learning</b>, Sutton and Barto, 2018
<ul>
<li>Available free online: <a href="http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html">http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html</a></li>

</ul></li>
<li><b>Algorithms for Reinforcement Learning</b>, Szepesvari, 2010
<ul>
<li>Available free online: <a href="http://www.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf">http://www.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf</a></li>

</ul></li>

</ul>
</section>
</section>
<section>
<section id="slide-org8673b32">
<h2 id="org8673b32">About RL</h2>
<div class="outline-text-2" id="text-org8673b32">
</div>
</section>
<section id="slide-org7259f52">
<h3 id="org7259f52">Many Faces of Reinforcement Learning</h3>

<div id="orgcf4f751" class="figure">
<p><img src="rl-to-discipline.svg" alt="rl-to-discipline.svg" class="org-svg" height="400px" />
</p>
</div>
</section>
<section id="slide-org13bd9fd">
<h3 id="org13bd9fd">Branches of Machine Learning</h3>

<div id="org4e25a3a" class="figure">
<p><img src="ml-types.svg" alt="ml-types.svg" class="org-svg" height="400px" />
</p>
</div>
</section>
<section id="slide-org9918fdd">
<h3 id="org9918fdd">Characteristics of Reinforcement Learning</h3>
<p>
What makes reinforcement learning different from other machine
learning paradigms?
</p>
<ul>
<li>No supervisor, only a <i>reward</i> signal</li>
<li>Feedback is delayed, not instantaneous</li>
<li>Time really matters (sequential, non i.i.d. data)</li>
<li>Agent‚Äôs actions affect subsequent data it receives</li>

</ul>
</section>
<section id="slide-orgf2913cf">
<h3 id="orgf2913cf">Examples (from 10 years ago)</h3>
<ul>
<li>Fly stunt manoeuvres in a helicopter</li>
<li>Defeat the world champion at Backgammon</li>
<li>Manage an investment portfolio</li>
<li>Control a power station</li>
<li>Make a humanoid robot walk</li>
<li>Play many different Atari games better than humans</li>

</ul>
</section>
<section id="slide-org9d461ac">
<h3 id="org9d461ac">Tesauro's TD Gammon (1992)</h3>

<div id="org30f9a42" class="figure">
<p><img src="td-gammon.png" alt="td-gammon.png" />
</p>
</div>
<aside class="notes">
<ul>
<li>first superhuman game play by RL</li>

<li>Backgammon is a game of chance but involves some decision making</li>

<li>TD Gammon is now reference for best move to make</li>

</ul>

</aside>
</section>
<section id="slide-orgd5bcc7d">
<h3 id="orgd5bcc7d">Fly stunt manoeuvres in a helicopter (2006)</h3>
<video controls muted data-autoplay src="Stanford Autonomous Helicopter - Chaos-kN6ifrqwIMY.mp4"> </video>
<aside class="notes">
<ul>
<li>How would you control a helicopter to perform this stunt?</li>

</ul>

</aside>
</section>
<section id="slide-org4936e73">
<h3 id="org4936e73">Atari DQN (Google DeepMind 2016) - Start of DeepRL</h3>
<video data-autoplay src="DQN Breakout-TmPfTpjtdgg.mp4" width="640" height="480"></video>
<aside class="notes">
<ul>
<li>prior to this - full access to internal state</li>
<li>this RL agent just sees pixel values</li>
<li>makes use of deep learning via experience replay</li>
<li>achieves superhuman performance</li>

</ul>

</aside>
</section>
<section id="slide-orge04f1cc">
<h3 id="orge04f1cc">Examples (more recently)</h3>
<ul>
<li>AlphaGo - defeat world champion at Go</li>
<li>Control cooling for a data centre</li>
<li>Find faster matrix multiplication algorithms</li>
<li>Learn to walk (robot dog) in 1 hour</li>
<li>Play StarCraft 2 better than humans</li>
<li>Drive Gran Turismo better than humans</li>
<li>Race drones faster than humans</li>
<li>Teach LLMs how to behave (RLHF)</li>

</ul>
</section>
<section id="slide-orgcdc7b5c">
<h3 id="orgcdc7b5c">Quick exercise (5 mins)</h3>
<ul>
<li>Go and find web pages, videos, or research papers for as many of the examples as you can</li>
<li>If you don't have a laptop, use your phone</li>
<li>If you don't have anything, help a neighbour</li>

</ul>
</section>
</section>
<section>
<section id="slide-orgaf9ae1c">
<h2 id="orgaf9ae1c">The RL Problem</h2>
<div class="outline-text-2" id="text-orgaf9ae1c">
</div>
</section>
<section id="slide-org355f413">
<h3 id="org355f413">Reward</h3>
<ul>
<li>A reward \(R_t\) is a scalar feedback signal
<ul>
<li>Indicates how well agent is doing at step \(t\)</li>

</ul></li>
<li>The agent‚Äôs job is to maximise cumulative reward</li>
<li>Reinforcement learning is based on the <b>reward hypothesis</b></li>

</ul>
<div class="definition" id="orgf913c65">
<p>
All goals can be described by the maximisation of expected cumulative reward
</p>

</div>
<p>
Do you agree?
</p>
</section>
<section id="slide-orgb3a1d78">
<h3 id="orgb3a1d78">Examples of Rewards</h3>
<ul>
<li>Fly stunt manoeuvres in a helicopter
<ul>
<li>+ve reward for following desired trajectory</li>
<li>-ve reward for crashing</li>

</ul></li>
<li>Defeat the world champion at Backgammon
<ul>
<li>+/-ve reward for winning/losing a game</li>

</ul></li>
<li>Manage an investment portfolio
<ul>
<li>+ve reward for each $ in bank</li>

</ul></li>
<li>Control a power station
<ul>
<li>+ve reward for producing power</li>
<li>-ve reward for exceeding safety thresholds</li>

</ul></li>
<li>Make a humanoid robot walk
<ul>
<li>+ve reward for forward motion</li>
<li>-ve reward for falling over</li>

</ul></li>
<li>Play many different Atari games better than humans
<ul>
<li>+/-ve reward for increasing/decreasing score</li>

</ul></li>

</ul>
</section>
<section id="slide-org16976ae">
<h3 id="org16976ae">Sequential Decision Making</h3>
<ul>
<li>Goal: <i>select actions to maximise total future reward</i></li>
<li>Actions may have long term consequences</li>
<li>Reward may be delayed</li>
<li>It may be better to sacrifice immediate reward to gain more long-term reward</li>
<li>Examples:
<ul>
<li>A financial investment (may take months to mature)</li>
<li>Refuelling a helicopter (might prevent a crash in several hours)</li>
<li>Blocking opponent moves (might help winning chances many moves from now)</li>

</ul></li>

</ul>
</section>
<section id="slide-orgbeed85f">
<h3 id="orgbeed85f">Environments (Agent and Environment)</h3>

<div id="orga36d3ab" class="figure">
<p><img src="agent-env.svg" alt="agent-env.svg" class="org-svg" height="400px" />
</p>
</div>
</section>
<section id="slide-org183d8c0">
<h3 id="org183d8c0">Environments (Agent and Environment)</h3>
<div style="display: flex;"><div style="flex: 1; padding: 0px;">


<div id="orgfc9dd8a" class="figure">
<p><img src="real-agent-env.svg" alt="real-agent-env.svg" class="org-svg" height="400px" />
</p>
</div>


</div><div style="flex: 1; padding: 0px; ">
<ul>
<li>At each step \(t\) the agent:
<ul>
<li>Executes action \(A_t\)</li>
<li>Receives observation \(O_t\)</li>
<li>Receives scalar reward \(R_t\)</li>

</ul></li>
<li>The environment:
<ul>
<li>Receives action \(A_t\)</li>
<li>Emits observation \(O_{t+1}\)</li>
<li>Emits scalar reward \(R_{t+1}\)</li>

</ul></li>
<li>\(t\) increments at environment step</li>

</ul>

</div></div>
</section>
<section id="slide-orgc38f164">
<h3 id="orgc38f164">State (History and State)</h3>
<ul>
<li>History is the sequence of observations, actions, rewards:
<ul>
<li>\(H_t = O_1, R_1, A_1, \ldots, A_{t-1}, O_t, R_t\)</li>

</ul></li>
<li>i.e. all observable variables up to time \(t\)</li>
<li>i.e. sensorimotor stream of a robot/embodied agent</li>
<li>What happens next depends on the history:
<ul>
<li>agent selects actions</li>
<li>environment selects observations/rewards</li>

</ul></li>
<li>State is the information used to determine what happens next</li>
<li>Formally, state is a function of the history:
<ul>
<li>\(S_t = f(H_t)\)</li>

</ul></li>

</ul>
</section>
<section id="slide-org8e97c1f">
<h3 id="org8e97c1f">State (Environment State)</h3>
<ul>
<li>The environment state \(S^e_t\) is the environment‚Äôs private representation
<ul>
<li>i.e. whatever data the environment uses to pick next observation/reward</li>

</ul></li>
<li>Environment state is not usually visible to the agent</li>
<li>Even if \(S^e_t\) is visible, it may contain irrelevant information</li>
<li>(Diagram on page 19)</li>

</ul>
</section>
<section id="slide-org4e06a5d">
<h3 id="org4e06a5d">State (Agent State)</h3>
<ul>
<li>The agent state \(S^a_t\) is the agent‚Äôs internal representation
<ul>
<li>i.e. whatever information the agent uses to pick the next action</li>
<li>i.e. the information used by RL algorithms</li>

</ul></li>
<li>It can be any function of history:
<ul>
<li>\(S^a_t = f(H_t)\)</li>

</ul></li>
<li>(Diagram on page 20)</li>

</ul>
</section>
<section id="slide-org4d5cf0b">
<h3 id="org4d5cf0b">State (Information / Markov State)</h3>
<ul>
<li>An information state (a.k.a. Markov state) contains all useful information from history.</li>

</ul>
<div class="definition" id="org42af091">
<p>
a state \(S_t\) is Markov iff
\[
   \mathbb{P}[S_{t+1}\mid S_t] = \mathbb{P}[S_{t+1}\mid S_1,\ldots,S_t]
\]
</p>

</div>
<ul>
<li>‚ÄúThe future is independent of the past given the present‚Äù
\[
  H_{1:t} \rightarrow S_t \rightarrow H_{t+1:\infty}
  \]</li>
<li>Once the state is known, the history may be thrown away
<ul>
<li>i.e. state is a sufficient statistic of the future</li>

</ul></li>
<li>Environment state \(S^e_t\) is Markov</li>
<li>History \(H_t\) is Markov</li>

</ul>
</section>
<section id="slide-orgb4dfaaa">
<h3 id="orgb4dfaaa">State (Rat Example)</h3>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">üí°</td>
<td class="org-left">üí°</td>
<td class="org-left">üïπÔ∏èÔ∏è</td>
<td class="org-left">üîî</td>
<td class="org-left">&nbsp;</td>
<td class="org-left">‚ö°</td>
</tr>

<tr>
<td class="org-left">üîî</td>
<td class="org-left">üí°</td>
<td class="org-left">üïπÔ∏è</td>
<td class="org-left">üïπÔ∏è</td>
<td class="org-left">&nbsp;</td>
<td class="org-left">üßÄ</td>
</tr>

<tr>
<td class="org-left">üïπÔ∏è</td>
<td class="org-left">üí°</td>
<td class="org-left">üïπÔ∏è</td>
<td class="org-left">üîî</td>
<td class="org-left">&nbsp;</td>
<td class="org-left">?</td>
</tr>
</tbody>
</table>

<ul>
<li>What if agent state = last 3 items in sequence?</li>
<li>What if agent state = counts for lights, bells and levers?</li>
<li>What if agent state = complete sequence?</li>

</ul>
</section>
<section id="slide-org8251c28">
<h3 id="org8251c28">State (Fully Observable Environments)</h3>
<div style="display: flex;"><div style="flex: 1; padding: 0px;">


<div id="org52a4029" class="figure">
<p><img src="real-agent-env.svg" alt="real-agent-env.svg" class="org-svg" height="400px" />
</p>
</div>


</div><div style="flex: 1; padding: 0px; ">
<ul>
<li><b>Full observability</b>: agent <b>directly</b> observes environment state
\[
  O_t = S^a_t = S^e_t
  \]</li>
<li>Agent state = environment state = information state</li>
<li>Formally: <b>Markov decision process (MDP)</b>
<ul>
<li>(Next lecture and the majority of this course)</li>

</ul></li>

</ul>

</div></div>
</section>
<section id="slide-org8423364">
<h3 id="org8423364">State (Partially Observable Environments)</h3>
<ul>
<li><b>Partial observability</b>: agent <b>indirectly</b> observes environment:
<ul>
<li>robot with camera vision isn‚Äôt told absolute location</li>
<li>trading agent only observes current prices</li>
<li>poker-playing agent only observes public cards</li>

</ul></li>
<li>Now agent state \(\neq\) environment state</li>
<li>Formally: <b>partially observable Markov decision process</b> (POMDP)</li>
<li>Agent must construct its own state representation \(S^a_t\), e.g.
<ul>
<li>Complete history: \(S^a_t = H_t\)</li>
<li>Beliefs of environment state:
<ul>
<li>\(S^a_t = (\mathbb{P}[S^e_t=s_1], \ldots, \mathbb{P}[S^e_t=s_n])\)</li>

</ul></li>
<li>Recurrent neural network:
<ul>
<li>\(S^a_t = \sigma(S^a_{t-1} W_s + O_t W_o)\)</li>

</ul></li>

</ul></li>

</ul>
</section>
</section>
<section>
<section id="slide-org95458d8">
<h2 id="org95458d8">Inside An RL Agent</h2>
<div class="outline-text-2" id="text-org95458d8">
</div>
</section>
<section id="slide-orgf27f6fc">
<h3 id="orgf27f6fc">Major Components</h3>
<ul>
<li>An RL agent may include one or more of:
<ul>
<li>Policy: agent‚Äôs behaviour function</li>
<li>Value function: how good is each state and/or action</li>
<li>Model: agent‚Äôs representation of the environment</li>

</ul></li>

</ul>
</section>
<section id="slide-orgf0b96c5">
<h3 id="orgf0b96c5">Policy</h3>
<ul>
<li>A policy is the agent‚Äôs behaviour</li>
<li>Map from state to action, e.g.
<ul>
<li>Deterministic policy: \(a = \pi(s)\)</li>
<li>Stochastic policy: \(\pi(a\mid s)=\mathbb{P}[A_t=a\mid S_t=s]\)</li>

</ul></li>

</ul>
</section>
<section id="slide-orgb2af170">
<h3 id="orgb2af170">Value Function</h3>
<ul>
<li>Value function is a prediction of future reward</li>
<li>Used to evaluate goodness/badness of states</li>
<li>Therefore to select between actions, e.g.
<ul>
<li>\(v^\pi(s) = \mathbb{E}_\pi[ R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots \mid S_t=s]\)</li>

</ul></li>

</ul>
</section>
<section id="slide-orgc0d7102">
<h3 id="orgc0d7102">Example (Value Function in Atari)</h3>
<ul>
<li>(Image-only slide; see page 28)</li>

</ul>
</section>
<section id="slide-orgafca83e">
<h3 id="orgafca83e">Model</h3>
<ul>
<li>A model predicts what the environment will do next
<ul>
<li>\(\mathcal{P}\) predicts next state</li>
<li>\(\mathcal{R}\) predicts next (immediate) reward
$$</li>
<li>\(\mathcal{P}^a_{ss'} = \mathbb{P}[S_{t+1}=s' \mid S_t=s, A_t=a]\)</li>
<li>\(\mathcal{R}^a_s = \mathbb{E}[R_{t+1} \mid S_t=s, A_t=a]\)</li>

</ul></li>

</ul>
</section>
<section id="slide-org215db4b">
<h3 id="org215db4b">Maze Example (Start)</h3>
<div style="display: flex;"><div style="flex: 1; padding: 0px;">

<div id="orgcb04d1f" class="figure">
<p><img src="maze1.svg" alt="maze1.svg" class="org-svg" height="400px" />
</p>
</div>

</div><div style="flex: 1; padding: 0px; ">
<ul>
<li>Rewards: \(-1\) per time-step</li>
<li>Actions: N, E, S, W</li>
<li>States: agent‚Äôs location</li>

</ul>

</div></div>
</section>
<section id="slide-orgffb7961">
<h3 id="orgffb7961">Maze Example (Policy)</h3>

<div id="org63a1844" class="figure">
<p><img src="maze2.svg" alt="maze2.svg" class="org-svg" height="400px" />
</p>
</div>

<ul>
<li>Arrows represent policy \(\pi(s)\) for each state \(s\)</li>

</ul>
</section>
<section id="slide-orgc133eb9">
<h3 id="orgc133eb9">Maze Example (Value Function)</h3>

<div id="org3c07ec2" class="figure">
<p><img src="maze3.svg" alt="maze3.svg" class="org-svg" height="400px" />
</p>
</div>

<ul>
<li>Numbers represent value \(v_\pi(s)\) of each state \(s\)</li>
<li>What is the value for the square marked ?</li>

</ul>
</section>
<section id="slide-org8db39a9">
<h3 id="org8db39a9">Maze Example (Model)</h3>
<div style="display: flex;"><div style="flex: 1; padding: 0px;">

<div id="orgd4a1694" class="figure">
<p><img src="maze4.svg" alt="maze4.svg" class="org-svg" height="400px" />
</p>
</div>

</div><div style="flex: 1; padding: 0px; ">
<ul>
<li>Agent may have an internal model of the environment
<ul>
<li>Dynamics: how actions change the state</li>
<li>Rewards: how much reward from each state</li>

</ul></li>
<li>The model may be imperfect</li>

</ul>

</div></div>
<ul>
<li>Grid layout represents transition model \(\mathcal{P}^a_{ss'}\)</li>
<li>Numbers represent immediate reward \(\mathcal{R}^a_s\) from each state \(s\) (same for all \(a\))</li>

</ul>
</section>
<section id="slide-org4da724f">
<h3 id="org4da724f">Categorizing RL agents (1)</h3>
<ul>
<li>Value-Based
<ul>
<li>No policy (implicit)</li>
<li>Value function</li>

</ul></li>
<li>Policy-Based
<ul>
<li>Policy</li>
<li>No value function</li>

</ul></li>
<li>Actor-Critic
<ul>
<li>Policy</li>
<li>Value function</li>

</ul></li>

</ul>
</section>
<section id="slide-org8c0d1fa">
<h3 id="org8c0d1fa">Categorizing RL agents (2)</h3>
<ul>
<li>Model-Free
<ul>
<li>Policy and/or value function</li>
<li>No model</li>

</ul></li>
<li>Model-Based
<ul>
<li>Policy and/or value function</li>
<li>Model</li>

</ul></li>

</ul>
</section>
<section id="slide-org2dd109f">
<h3 id="org2dd109f">RL Agent Taxonomy</h3>
<ul>
<li>(Diagram on page 36: Model / Value Function / Policy overlap; value-based vs policy-based; model-free vs model-based; actor-critic in intersection)</li>

</ul>
</section>
</section>
<section>
<section id="slide-org075d0cc">
<h2 id="org075d0cc">Problems within RL</h2>
<div class="outline-text-2" id="text-org075d0cc">
</div>
</section>
<section id="slide-orgcebfa5c">
<h3 id="orgcebfa5c">Learning and Planning</h3>
<ul>
<li>Two fundamental problems in sequential decision making</li>
<li>Reinforcement Learning:
<ul>
<li>environment initially unknown</li>
<li>agent interacts with environment</li>
<li>agent improves its policy</li>

</ul></li>
<li>Planning:
<ul>
<li>model of environment is known</li>
<li>agent performs computations with model (no external interaction)</li>
<li>agent improves its policy</li>
<li>a.k.a. deliberation, reasoning, introspection, pondering, thought, search</li>

</ul></li>

</ul>
</section>
<section id="slide-org2ebd4ca">
<h3 id="org2ebd4ca">Atari Example (Reinforcement Learning)</h3>
<ul>
<li>Rules of the game are unknown</li>
<li>Learn directly from interactive gameplay</li>
<li>Pick actions on joystick, see pixels and scores</li>
<li>(Diagram on page 38)</li>

</ul>
</section>
<section id="slide-org2f66c53">
<h3 id="org2f66c53">Atari Example (Planning)</h3>
<ul>
<li>Rules of the game are known</li>
<li>Can query emulator
<ul>
<li>perfect model inside agent‚Äôs brain</li>

</ul></li>
<li>If I take action \(a\) from state \(s\):
<ul>
<li>what would the next state be?</li>
<li>what would the score be?</li>

</ul></li>
<li>Plan ahead to find optimal policy
<ul>
<li>e.g. tree search</li>

</ul></li>
<li>(Diagram on page 39)</li>

</ul>
</section>
<section id="slide-org340b179">
<h3 id="org340b179">Exploration and Exploitation (1)</h3>
<ul>
<li>RL is like trial-and-error learning</li>
<li>Agent should discover a good policy
<ul>
<li>from experiences of the environment</li>
<li>without losing too much reward along the way</li>

</ul></li>

</ul>
</section>
<section id="slide-org78a77f7">
<h3 id="org78a77f7">Exploration and Exploitation (2)</h3>
<ul>
<li>Exploration finds more information about the environment</li>
<li>Exploitation exploits known information to maximise reward</li>
<li>Usually important to explore as well as exploit</li>

</ul>
</section>
<section id="slide-orgf6a6a7c">
<h3 id="orgf6a6a7c">Examples (Explore/Exploit)</h3>
<ul>
<li>Restaurant selection
<ul>
<li>Exploitation: go to favourite restaurant</li>
<li>Exploration: try a new restaurant</li>

</ul></li>
<li>Online banner advertisements
<ul>
<li>Exploitation: show most successful advert</li>
<li>Exploration: show a different advert</li>

</ul></li>
<li>Oil drilling
<ul>
<li>Exploitation: drill at best known location</li>
<li>Exploration: drill at a new location</li>

</ul></li>
<li>Game playing
<ul>
<li>Exploitation: play the move believed best</li>
<li>Exploration: play an experimental move</li>

</ul></li>

</ul>
</section>
<section id="slide-orgd08d4fa">
<h3 id="orgd08d4fa">Prediction and Control</h3>
<ul>
<li>Prediction: evaluate the future (given a policy)</li>
<li>Control: optimise the future (find the best policy)</li>

</ul>
</section>
<section id="slide-orgfcc1bc6">
<h3 id="orgfcc1bc6">Gridworld Example (Prediction)</h3>
<ul>
<li>(Gridworld table + question on page 44)</li>
<li>Question: What is the value function for the uniform random policy?</li>

</ul>
</section>
<section id="slide-org7c4b4aa">
<h3 id="org7c4b4aa">Gridworld Example (Control)</h3>
<ul>
<li>(Figure on page 45: gridworld + \(V^*\) + \(\pi^*\))</li>
<li>Questions:
<ul>
<li>What is the optimal value function over all possible policies?</li>
<li>What is the optimal policy?</li>

</ul></li>

</ul>
</section>
</section>
<section>
<section id="slide-orgd7c1676">
<h2 id="orgd7c1676">Course Outline</h2>
<ul>
<li>Part I: Elementary Reinforcement Learning
<ol>
<li>Introduction to RL</li>
<li>Markov Decision Processes</li>
<li>Planning by Dynamic Programming</li>
<li>Model-Free Prediction</li>
<li>Model-Free Control</li>

</ol></li>
<li>Part II: Reinforcement Learning in Practice
<ol>
<li>Value Function Approximation</li>
<li>Policy Gradient Methods</li>
<li>Integrating Learning and Planning</li>
<li>Exploration and Exploitation</li>
<li>Case study ‚Äî RL in Car Climate Control</li>

</ol></li>

</ul>
</section>
</section>
</div>
</div>
<script src="https://cdn.jsdelivr.net/npm/reveal.js/dist/reveal.js"></script>
<script src="https://cdn.jsdelivr.net/npm/reveal.js/plugin/markdown/markdown.js"></script>
<script src="https://cdn.jsdelivr.net/npm/reveal.js/plugin/notes/notes.js"></script>
<script src="https://cdn.jsdelivr.net/npm/reveal.js/plugin/search/search.js"></script>
<script src="https://cdn.jsdelivr.net/npm/reveal.js/plugin/zoom/zoom.js"></script>
<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: 'c',
rollingLinks: false,
keyboard: true,
mouseWheel: false,
fragmentInURL: false,
hashOneBasedIndex: false,
pdfSeparateFragments: true,
overview: true,
margin: 0.10,
minScale: 0.10,
maxScale: 2.50,

transition: 'convex',
transitionSpeed: 'default',

// Plugins with reveal.js 4.x
plugins: [ RevealMarkdown, RevealNotes, RevealSearch, RevealZoom ],

// Optional libraries used to extend reveal.js
dependencies: [
]

});
</script>
</body>
</html>
