<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title>Lecture 1: Introduction to Reinforcement Learning</title>
<meta name="author" content="James Brusey"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js/dist/reveal.css"/>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js/dist/theme/serif.css" id="theme"/>

<link rel="stylesheet" href="custom.css"/>
<meta name="description" content="J Brusey - ">
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide">
<h1 class="title">Lecture 1: Introduction to Reinforcement Learning</h1><h2 class="author">James Brusey</h2>
</section>
<section>
<section id="slide-org6adec1e">
<h2 id="org6adec1e">Outline</h2>
<ol>
<li>Admin</li>
<li>About Reinforcement Learning</li>
<li>The Reinforcement Learning Problem</li>
<li>Inside An RL Agent</li>
<li>Problems within Reinforcement Learning</li>

</ol>
</section>
</section>
<section>
<section id="slide-orga70fae5">
<h2 id="orga70fae5">Admin: Textbooks</h2>
<ul>
<li><b>An Introduction to Reinforcement Learning</b>, Sutton and Barto, 2018
<ul>
<li>Available free online: <a href="http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html">http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html</a></li>

</ul></li>
<li><b>Algorithms for Reinforcement Learning</b>, Szepesvari, 2010
<ul>
<li>Available free online: <a href="http://www.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf">http://www.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf</a></li>

</ul></li>

</ul>
</section>
</section>
<section>
<section id="slide-orgb9ee0cc">
<h2 id="orgb9ee0cc">About RL</h2>
<div class="outline-text-2" id="text-orgb9ee0cc">
</div>
</section>
<section id="slide-orgb69371d">
<h3 id="orgb69371d">Many Faces of Reinforcement Learning</h3>

<div id="org1cf8922" class="figure">
<p><img src="rl-to-discipline.svg" alt="rl-to-discipline.svg" class="org-svg" height="400px" />
</p>
</div>
</section>
<section id="slide-org768963c">
<h3 id="org768963c">Branches of Machine Learning</h3>

<div id="org1ca3abc" class="figure">
<p><img src="ml-types.svg" alt="ml-types.svg" class="org-svg" height="400px" />
</p>
</div>
</section>
<section id="slide-org7214e46">
<h3 id="org7214e46">Characteristics of Reinforcement Learning</h3>
<p>
What makes reinforcement learning different from other machine
learning paradigms?
</p>
<ul>
<li>No supervisor, only a <i>reward</i> signal</li>
<li>Feedback is delayed, not instantaneous</li>
<li>Time really matters (sequential, non i.i.d. data)</li>
<li>Agent’s actions affect subsequent data it receives</li>

</ul>
</section>
<section id="slide-org3a06033">
<h3 id="org3a06033">Examples (from 10 years ago)</h3>
<ul>
<li>Fly stunt manoeuvres in a helicopter</li>
<li>Defeat the world champion at Backgammon</li>
<li>Manage an investment portfolio</li>
<li>Control a power station</li>
<li>Make a humanoid robot walk</li>
<li>Play many different Atari games better than humans</li>

</ul>
</section>
<section id="slide-orgf9961ce">
<h3 id="orgf9961ce">Tesauro's TD Gammon (1992)</h3>

<div id="org716a6a7" class="figure">
<p><img src="figures/td-gammon.png" alt="td-gammon.png" />
</p>
</div>
<aside class="notes">
<ul>
<li>first superhuman game play by RL</li>

<li>Backgammon is a game of chance but involves some decision making</li>

<li>TD Gammon is now reference for best move to make</li>

</ul>

</aside>
</section>
<section id="slide-orgc061299">
<h3 id="orgc061299">Fly stunt manoeuvres in a helicopter (2006)</h3>
<video controls muted data-autoplay src="Stanford Autonomous Helicopter - Chaos-kN6ifrqwIMY.mp4"> </video>
<aside class="notes">
<ul>
<li>How would you control a helicopter to perform this stunt?</li>

</ul>

</aside>
</section>
<section id="slide-orgbe954aa">
<h3 id="orgbe954aa">Atari DQN (Google DeepMind 2016) - Start of DeepRL</h3>
<video data-autoplay src="../2023-02-rl/figures/DQN Breakout-TmPfTpjtdgg.mp4" width="640" height="480"></video>
<aside class="notes">
<ul>
<li>prior to this - full access to internal state</li>
<li>this RL agent just sees pixel values</li>
<li>makes use of deep learning via experience replay</li>
<li>achieves superhuman performance</li>

</ul>

</aside>
</section>
<section id="slide-org534949c">
<h3 id="org534949c">Examples (more recently)</h3>
<ul>
<li>AlphaGo - defeat world champion at Go</li>
<li>Control cooling for a data centre</li>
<li>Find faster matrix multiplication algorithms</li>
<li>Learn to walk (robot dog) in 1 hour</li>
<li>Play StarCraft 2 better than humans</li>
<li>Drive Gran Turismo better than humans</li>
<li>Race drones faster than humans</li>
<li>Teach LLMs how to behave (RLHF)</li>

</ul>
</section>
<section id="slide-orgc7d3eb2">
<h3 id="orgc7d3eb2">Quick exercise</h3>
<ul>
<li>Go and find web pages, videos, or research papers for as many of the examples as you can</li>
<li>If you don't have a laptop, use your phone</li>
<li>If you don't have anything, help a neighbour</li>

</ul>
</section>
</section>
<section>
<section id="slide-org1acc23c">
<h2 id="org1acc23c">The RL Problem: Reward</h2>
<ul>
<li>A reward \(R_t\) is a scalar feedback signal
<ul>
<li>Indicates how well agent is doing at step \(t\)</li>

</ul></li>
<li>The agent’s job is to maximise cumulative reward</li>
<li>Reinforcement learning is based on the <b>reward hypothesis</b></li>
<li>Definition (Reward Hypothesis)
<ul>
<li>All goals can be described by the maximisation of expected cumulative reward</li>

</ul></li>
<li>Question: Do you agree with this statement?</li>

</ul>
</section>
</section>
<section>
<section id="slide-org33db75f">
<h2 id="org33db75f">The RL Problem: Examples of Rewards</h2>
<ul>
<li>Fly stunt manoeuvres in a helicopter
<ul>
<li>+ve reward for following desired trajectory</li>
<li>-ve reward for crashing</li>

</ul></li>
<li>Defeat the world champion at Backgammon
<ul>
<li>+/-ve reward for winning/losing a game</li>

</ul></li>
<li>Manage an investment portfolio
<ul>
<li>+ve reward for each $ in bank</li>

</ul></li>
<li>Control a power station
<ul>
<li>+ve reward for producing power</li>
<li>-ve reward for exceeding safety thresholds</li>

</ul></li>
<li>Make a humanoid robot walk
<ul>
<li>+ve reward for forward motion</li>
<li>-ve reward for falling over</li>

</ul></li>
<li>Play many different Atari games better than humans
<ul>
<li>+/-ve reward for increasing/decreasing score</li>

</ul></li>

</ul>
</section>
</section>
<section>
<section id="slide-orgbd6ae23">
<h2 id="orgbd6ae23">The RL Problem: Sequential Decision Making</h2>
<ul>
<li>Goal: select actions to maximise total future reward</li>
<li>Actions may have long term consequences</li>
<li>Reward may be delayed</li>
<li>It may be better to sacrifice immediate reward to gain more long-term reward</li>
<li>Examples:
<ul>
<li>A financial investment (may take months to mature)</li>
<li>Refuelling a helicopter (might prevent a crash in several hours)</li>
<li>Blocking opponent moves (might help winning chances many moves from now)</li>

</ul></li>

</ul>
</section>
</section>
<section>
<section id="slide-orga7f2319">
<h2 id="orga7f2319">The RL Problem: Environments (Agent and Environment)</h2>
<ul>
<li>(Diagram on page 16: agent/environment with observation \(O_t\), action \(A_t\), reward \(R_t\))</li>

</ul>
</section>
</section>
<section>
<section id="slide-org105f502">
<h2 id="org105f502">The RL Problem: Environments (Agent and Environment)</h2>
<ul>
<li>At each step \(t\) the agent:
<ul>
<li>Executes action \(A_t\)</li>
<li>Receives observation \(O_t\)</li>
<li>Receives scalar reward \(R_t\)</li>

</ul></li>
<li>The environment:
<ul>
<li>Receives action \(A_t\)</li>
<li>Emits observation \(O_{t+1}\)</li>
<li>Emits scalar reward \(R_{t+1}\)</li>

</ul></li>
<li>\(t\) increments at environment step</li>
<li>(Same diagram, with bullets; page 17)</li>

</ul>
</section>
</section>
<section>
<section id="slide-orgdb0434d">
<h2 id="orgdb0434d">The RL Problem: State (History and State)</h2>
<ul>
<li>History is the sequence of observations, actions, rewards:
<ul>
<li>\(H_t = O_1, R_1, A_1, \ldots, A_{t-1}, O_t, R_t\)</li>

</ul></li>
<li>i.e. all observable variables up to time \(t\)</li>
<li>i.e. sensorimotor stream of a robot/embodied agent</li>
<li>What happens next depends on the history:
<ul>
<li>agent selects actions</li>
<li>environment selects observations/rewards</li>

</ul></li>
<li>State is the information used to determine what happens next</li>
<li>Formally, state is a function of the history:
<ul>
<li>\(S_t = f(H_t)\)</li>

</ul></li>

</ul>
</section>
</section>
<section>
<section id="slide-orgf28626b">
<h2 id="orgf28626b">The RL Problem: State (Environment State)</h2>
<ul>
<li>The environment state \(S^e_t\) is the environment’s private representation
<ul>
<li>i.e. whatever data the environment uses to pick next observation/reward</li>

</ul></li>
<li>Environment state is not usually visible to the agent</li>
<li>Even if \(S^e_t\) is visible, it may contain irrelevant information</li>
<li>(Diagram on page 19)</li>

</ul>
</section>
</section>
<section>
<section id="slide-org56e9d2b">
<h2 id="org56e9d2b">The RL Problem: State (Agent State)</h2>
<ul>
<li>The agent state \(S^a_t\) is the agent’s internal representation
<ul>
<li>i.e. whatever information the agent uses to pick the next action</li>
<li>i.e. the information used by RL algorithms</li>

</ul></li>
<li>It can be any function of history:
<ul>
<li>\(S^a_t = f(H_t)\)</li>

</ul></li>
<li>(Diagram on page 20)</li>

</ul>
</section>
</section>
<section>
<section id="slide-org9cb1f76">
<h2 id="org9cb1f76">The RL Problem: State (Information / Markov State)</h2>
<ul>
<li>An information state (a.k.a. Markov state) contains all useful information from history.</li>
<li>Definition: a state \(S_t\) is Markov iff
<ul>
<li>\(P[S_{t+1}\mid S_t] = P[S_{t+1}\mid S_1,\ldots,S_t]\)</li>

</ul></li>
<li>“The future is independent of the past given the present”
<ul>
<li>\(H_{1:t} \rightarrow S_t \rightarrow H_{t+1:\infty}\)</li>

</ul></li>
<li>Once the state is known, the history may be thrown away
<ul>
<li>i.e. state is a sufficient statistic of the future</li>

</ul></li>
<li>Environment state \(S^e_t\) is Markov</li>
<li>History \(H_t\) is Markov</li>

</ul>
</section>
</section>
<section>
<section id="slide-orgb51120b">
<h2 id="orgb51120b">The RL Problem: State (Rat Example)</h2>
<ul>
<li>What if agent state = last 3 items in sequence?</li>
<li>What if agent state = counts for lights, bells and levers?</li>
<li>What if agent state = complete sequence?</li>
<li>(Illustration on page 22)</li>

</ul>
</section>
</section>
<section>
<section id="slide-orgfccb5b8">
<h2 id="orgfccb5b8">The RL Problem: State (Fully Observable Environments)</h2>
<ul>
<li>Full observability: agent directly observes environment state
<ul>
<li>\(O_t = S^a_t = S^e_t\)</li>

</ul></li>
<li>Agent state = environment state = information state</li>
<li>Formally: Markov decision process (MDP)
<ul>
<li>(Next lecture and the majority of this course)</li>

</ul></li>
<li>(Diagram on page 23)</li>

</ul>
</section>
</section>
<section>
<section id="slide-org71a4943">
<h2 id="org71a4943">The RL Problem: State (Partially Observable Environments)</h2>
<ul>
<li>Partial observability: agent indirectly observes environment:
<ul>
<li>robot with camera vision isn’t told absolute location</li>
<li>trading agent only observes current prices</li>
<li>poker-playing agent only observes public cards</li>

</ul></li>
<li>Now agent state \(\neq\) environment state</li>
<li>Formally: partially observable Markov decision process (POMDP)</li>
<li>Agent must construct its own state representation \(S^a_t\), e.g.
<ul>
<li>Complete history: \(S^a_t = H_t\)</li>
<li>Beliefs of environment state:
<ul>
<li>\(S^a_t = (P[S^e_t=s_1], \ldots, P[S^e_t=s_n])\)</li>

</ul></li>
<li>Recurrent neural network:
<ul>
<li>\(S^a_t = \sigma(S^a_{t-1} W_s + O_t W_o)\)</li>

</ul></li>

</ul></li>

</ul>
</section>
</section>
<section>
<section id="slide-org27804ea">
<h2 id="org27804ea">Inside An RL Agent: Major Components</h2>
<ul>
<li>An RL agent may include one or more of:
<ul>
<li>Policy: agent’s behaviour function</li>
<li>Value function: how good is each state and/or action</li>
<li>Model: agent’s representation of the environment</li>

</ul></li>

</ul>
</section>
</section>
<section>
<section id="slide-org89c7e2c">
<h2 id="org89c7e2c">Inside An RL Agent: Policy</h2>
<ul>
<li>A policy is the agent’s behaviour</li>
<li>Map from state to action, e.g.
<ul>
<li>Deterministic policy: \(a = \pi(s)\)</li>
<li>Stochastic policy: \(\pi(a\mid s)=P[A_t=a\mid S_t=s]\)</li>

</ul></li>

</ul>
</section>
</section>
<section>
<section id="slide-org24d22d7">
<h2 id="org24d22d7">Inside An RL Agent: Value Function</h2>
<ul>
<li>Value function is a prediction of future reward</li>
<li>Used to evaluate goodness/badness of states</li>
<li>Therefore to select between actions, e.g.
<ul>
<li>\(v^\pi(s) = \mathbb{E}_\pi[ R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots \mid S_t=s]\)</li>

</ul></li>

</ul>
</section>
</section>
<section>
<section id="slide-org48ce50c">
<h2 id="org48ce50c">Inside An RL Agent: Example (Value Function in Atari)</h2>
<ul>
<li>(Image-only slide; see page 28)</li>

</ul>
</section>
</section>
<section>
<section id="slide-org134c988">
<h2 id="org134c988">Inside An RL Agent: Model</h2>
<ul>
<li>A model predicts what the environment will do next
<ul>
<li>\(P\) predicts next state</li>
<li>\(R\) predicts next (immediate) reward</li>

</ul></li>
<li>Example:
<ul>
<li>\(P^a_{ss'} = P[S_{t+1}=s' \mid S_t=s, A_t=a]\)</li>
<li>\(R^a_s = \mathbb{E}[R_{t+1} \mid S_t=s, A_t=a]\)</li>

</ul></li>

</ul>
</section>
</section>
<section>
<section id="slide-orgd645930">
<h2 id="orgd645930">Inside An RL Agent: Maze Example (Start)</h2>
<ul>
<li>Goal</li>
<li>Rewards: -1 per time-step</li>
<li>Actions: N, E, S, W</li>
<li>States: agent’s location</li>
<li>(Maze diagram on page 30)</li>

</ul>
</section>
</section>
<section>
<section id="slide-org3373cb0">
<h2 id="org3373cb0">Inside An RL Agent: Maze Example (Policy)</h2>
<ul>
<li>Arrows represent policy \(\pi(s)\) for each state \(s\)</li>
<li>(Diagram on page 31)</li>

</ul>
</section>
</section>
<section>
<section id="slide-org8eea3d9">
<h2 id="org8eea3d9">Inside An RL Agent: Maze Example (Value Function)</h2>
<ul>
<li>Numbers represent value \(v_\pi(s)\) of each state \(s\)</li>
<li>(Diagram on page 32)</li>

</ul>
</section>
</section>
<section>
<section id="slide-org0ace0d3">
<h2 id="org0ace0d3">Inside An RL Agent: Maze Example (Model)</h2>
<ul>
<li>Agent may have an internal model of the environment
<ul>
<li>Dynamics: how actions change the state</li>
<li>Rewards: how much reward from each state</li>

</ul></li>
<li>The model may be imperfect</li>
<li>Grid layout represents transition model \(P^a_{ss'}\)</li>
<li>Numbers represent immediate reward \(R^a_s\) from each state \(s\) (same for all \(a\))</li>
<li>(Diagram on page 33)</li>

</ul>
</section>
</section>
<section>
<section id="slide-org1eab8f0">
<h2 id="org1eab8f0">Inside An RL Agent: Categorizing RL agents (1)</h2>
<ul>
<li>Value-Based
<ul>
<li>No policy (implicit)</li>
<li>Value function</li>

</ul></li>
<li>Policy-Based
<ul>
<li>Policy</li>
<li>No value function</li>

</ul></li>
<li>Actor-Critic
<ul>
<li>Policy</li>
<li>Value function</li>

</ul></li>

</ul>
</section>
</section>
<section>
<section id="slide-org96f1db8">
<h2 id="org96f1db8">Inside An RL Agent: Categorizing RL agents (2)</h2>
<ul>
<li>Model-Free
<ul>
<li>Policy and/or value function</li>
<li>No model</li>

</ul></li>
<li>Model-Based
<ul>
<li>Policy and/or value function</li>
<li>Model</li>

</ul></li>

</ul>
</section>
</section>
<section>
<section id="slide-org56d4ff5">
<h2 id="org56d4ff5">Inside An RL Agent: RL Agent Taxonomy</h2>
<ul>
<li>(Diagram on page 36: Model / Value Function / Policy overlap; value-based vs policy-based; model-free vs model-based; actor-critic in intersection)</li>

</ul>
</section>
</section>
<section>
<section id="slide-orgbf1301c">
<h2 id="orgbf1301c">Problems within RL: Learning and Planning</h2>
<ul>
<li>Two fundamental problems in sequential decision making</li>
<li>Reinforcement Learning:
<ul>
<li>environment initially unknown</li>
<li>agent interacts with environment</li>
<li>agent improves its policy</li>

</ul></li>
<li>Planning:
<ul>
<li>model of environment is known</li>
<li>agent performs computations with model (no external interaction)</li>
<li>agent improves its policy</li>
<li>a.k.a. deliberation, reasoning, introspection, pondering, thought, search</li>

</ul></li>

</ul>
</section>
</section>
<section>
<section id="slide-org652eb65">
<h2 id="org652eb65">Problems within RL: Atari Example (Reinforcement Learning)</h2>
<ul>
<li>Rules of the game are unknown</li>
<li>Learn directly from interactive gameplay</li>
<li>Pick actions on joystick, see pixels and scores</li>
<li>(Diagram on page 38)</li>

</ul>
</section>
</section>
<section>
<section id="slide-org547fb97">
<h2 id="org547fb97">Problems within RL: Atari Example (Planning)</h2>
<ul>
<li>Rules of the game are known</li>
<li>Can query emulator
<ul>
<li>perfect model inside agent’s brain</li>

</ul></li>
<li>If I take action \(a\) from state \(s\):
<ul>
<li>what would the next state be?</li>
<li>what would the score be?</li>

</ul></li>
<li>Plan ahead to find optimal policy
<ul>
<li>e.g. tree search</li>

</ul></li>
<li>(Diagram on page 39)</li>

</ul>
</section>
</section>
<section>
<section id="slide-orgf81024c">
<h2 id="orgf81024c">Problems within RL: Exploration and Exploitation (1)</h2>
<ul>
<li>RL is like trial-and-error learning</li>
<li>Agent should discover a good policy
<ul>
<li>from experiences of the environment</li>
<li>without losing too much reward along the way</li>

</ul></li>

</ul>
</section>
</section>
<section>
<section id="slide-org2e8c113">
<h2 id="org2e8c113">Problems within RL: Exploration and Exploitation (2)</h2>
<ul>
<li>Exploration finds more information about the environment</li>
<li>Exploitation exploits known information to maximise reward</li>
<li>Usually important to explore as well as exploit</li>

</ul>
</section>
</section>
<section>
<section id="slide-orgc8af770">
<h2 id="orgc8af770">Problems within RL: Examples (Explore/Exploit)</h2>
<ul>
<li>Restaurant selection
<ul>
<li>Exploitation: go to favourite restaurant</li>
<li>Exploration: try a new restaurant</li>

</ul></li>
<li>Online banner advertisements
<ul>
<li>Exploitation: show most successful advert</li>
<li>Exploration: show a different advert</li>

</ul></li>
<li>Oil drilling
<ul>
<li>Exploitation: drill at best known location</li>
<li>Exploration: drill at a new location</li>

</ul></li>
<li>Game playing
<ul>
<li>Exploitation: play the move believed best</li>
<li>Exploration: play an experimental move</li>

</ul></li>

</ul>
</section>
</section>
<section>
<section id="slide-orgae56faa">
<h2 id="orgae56faa">Problems within RL: Prediction and Control</h2>
<ul>
<li>Prediction: evaluate the future (given a policy)</li>
<li>Control: optimise the future (find the best policy)</li>

</ul>
</section>
</section>
<section>
<section id="slide-org384c084">
<h2 id="org384c084">Problems within RL: Gridworld Example (Prediction)</h2>
<ul>
<li>(Gridworld table + question on page 44)</li>
<li>Question: What is the value function for the uniform random policy?</li>

</ul>
</section>
</section>
<section>
<section id="slide-org58b3b81">
<h2 id="org58b3b81">Problems within RL: Gridworld Example (Control)</h2>
<ul>
<li>(Figure on page 45: gridworld + \(V^*\) + \(\pi^*\))</li>
<li>Questions:
<ul>
<li>What is the optimal value function over all possible policies?</li>
<li>What is the optimal policy?</li>

</ul></li>

</ul>
</section>
</section>
<section>
<section id="slide-orgd4950b9">
<h2 id="orgd4950b9">Course Outline</h2>
<ul>
<li>Part I: Elementary Reinforcement Learning
<ol>
<li>Introduction to RL</li>
<li>Markov Decision Processes</li>
<li>Planning by Dynamic Programming</li>
<li>Model-Free Prediction</li>
<li>Model-Free Control</li>

</ol></li>
<li>Part II: Reinforcement Learning in Practice
<ol>
<li>Value Function Approximation</li>
<li>Policy Gradient Methods</li>
<li>Integrating Learning and Planning</li>
<li>Exploration and Exploitation</li>
<li>Case study — RL in Car Climate Control</li>

</ol></li>

</ul>
</section>
</section>
</div>
</div>
<script src="https://cdn.jsdelivr.net/npm/reveal.js/dist/reveal.js"></script>
<script src="https://cdn.jsdelivr.net/npm/reveal.js/plugin/markdown/markdown.js"></script>
<script src="https://cdn.jsdelivr.net/npm/reveal.js/plugin/notes/notes.js"></script>
<script src="https://cdn.jsdelivr.net/npm/reveal.js/plugin/search/search.js"></script>
<script src="https://cdn.jsdelivr.net/npm/reveal.js/plugin/zoom/zoom.js"></script>
<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: 'c',
rollingLinks: false,
keyboard: true,
mouseWheel: false,
fragmentInURL: false,
hashOneBasedIndex: false,
pdfSeparateFragments: true,
overview: true,
margin: 0.10,
minScale: 0.10,
maxScale: 2.50,

transition: 'convex',
transitionSpeed: 'default',

// Plugins with reveal.js 4.x
plugins: [ RevealMarkdown, RevealNotes, RevealSearch, RevealZoom ],

// Optional libraries used to extend reveal.js
dependencies: [
]

});
</script>
</body>
</html>
