#+Title:  Model-Free Reinforcement Learning
#+Author: James Brusey
#+Date: 
#+Email: james.brusey@coventry.ac.uk
#+Options: toc:nil num:nil timestamp:nil tex:mathjax
#+REVEAL_WIDTH: 1200
#+REVEAL_HEIGHT: 800
#+REVEAL_MARGIN: 0.1
#+REVEAL_MIN_SCALE: 0.1
#+REVEAL_MAX_SCALE: 2.5
#+REVEAL_TRANSITION: slide
#+REVEAL_SLIDE_NUMBER: t
#+REVEAL_THEME: serif
#+REVEAL_HEAD_PREAMBLE: <meta name="description" content="J Brusey - ">
#+REVEAL_MATHJAX_URL: https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js
#+REVEAL_EXTRA_CSS: custom.css
#+REVEAL_HLEVEL: 1

* Introduction
** Model-Free Reinforcement Learning

- Last lecture: planning by dynamic programming

  * Solve a known Markov Decision Process
- This lecture: model-free prediction

  * Estimate the value function of an unknown Markov Decision Process
- Next lecture: model-free control

  * Optimise the value function of an unknown Markov Decision Process

* Monte-Carlo Learning
  ** Monte-Carlo Reinforcement Learning

- Learn directly from episodes of experience
- Model-free: no knowledge of transitions or rewards
- Learn from complete episodes (no bootstrapping)
- Value = mean return
- Only applicable to episodic tasks (must terminate)

** Monte-Carlo Policy Evaluation
Goal: learn (v_\pi) from episodes generated by policy (\pi)

Episode:
\[
S_1,A_1,R_2,\dots,S_T
\]

Return:
\[
G_t = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{T-1} R_T
\]

Value function:
\[
v_\pi(s) = \mathbb{E}_\pi[G_t \mid S_t=s]
\]

Monte-Carlo uses empirical mean of returns.

** First-Visit Monte-Carlo
For state (s):

* Let (t) be the first time (s) appears in an episode
* (N(s)\leftarrow N(s)+1)
* (S(s)\leftarrow S(s)+G_t)
* (V(s)=S(s)/N(s))

By the law of large numbers:
\[
V(s)\to v_\pi(s)\ \text{as }N(s)\to\infty
\]

** Every-Visit Monte-Carlo
For every visit of (s) at time (t):

* (N(s)\leftarrow N(s)+1)
* (S(s)\leftarrow S(s)+G_t)
* (V(s)=S(s)/N(s))

Again (V(s)\to v_\pi(s)).

** Blackjack Example

* State:

  * Player sum (12!-!21)
  * Dealer showing card (1!-!10)
  * Usable ace? yes/no
* Actions:

  * *stick*: stop and terminate
  * *twist*: take another card
* Rewards:

  * stick: (+1,0,-1) depending on win/draw/lose
  * twist: (-1) if bust, else (0)
* Automatic twist if sum < 12

#+BEGIN_NOTE
Original slide contains photo of a blackjack table.
#+END_NOTE

** Blackjack Value Function
Policy: *stick if sum ≥ 20, else twist*

#+BEGIN_NOTE
Diagram: 3-D value surfaces after 10k and 500k episodes,
for usable / non-usable ace.
#+END_NOTE

** Incremental Mean
For data (x_1,\dots,x_k)
\[
\mu_k=\frac{1}{k}\sum_{j=1}^k x_j
=\mu_{k-1}+\frac{1}{k}(x_k-\mu_{k-1})
\]

** Incremental Monte-Carlo
For each visit to state (S_t) with return (G_t):
\[
N(S_t)\leftarrow N(S_t)+1
\]
\[
V(S_t)\leftarrow V(S_t)+\frac{1}{N(S_t)}(G_t-V(S_t))
\]

With constant step-size:
\[
V(S_t)\leftarrow V(S_t)+\alpha(G_t-V(S_t))
\]

* Temporal-Difference Learning
  ** Temporal-Difference Learning

- Learn directly from experience
- Model-free
- Learn from incomplete episodes (bootstrapping)
- Update a guess towards a guess

** MC vs TD
Monte-Carlo:
\[
V(S_t)\leftarrow V(S_t)+\alpha(G_t-V(S_t))
\]

TD(0):
\[
V(S_t)\leftarrow V(S_t)+\alpha\big(R_{t+1}+\gamma V(S_{t+1})-V(S_t)\big)
\]

TD target:
\[
R_{t+1}+\gamma V(S_{t+1})
\]

TD error:
\[
\delta_t=R_{t+1}+\gamma V(S_{t+1})-V(S_t)
\]

** Driving Home Example
#+BEGIN_NOTE
Table of states: leaving office → reach car → exit highway → behind truck → home street → arrive home,
with predicted and actual travel times.
#+END_NOTE

** Driving Home: MC vs TD
#+BEGIN_NOTE
Two plots showing how MC and TD(0) change value estimates over time.
#+END_NOTE

** MC vs TD: Pros/Cons

* TD can learn before final outcome
* TD learns online
* TD works in continuing tasks
* MC needs full episodes
* MC only works in episodic tasks

** Bias–Variance
Return:
\[
G_t=R_{t+1}+\gamma R_{t+2}+\dots+\gamma^{T-1}R_T
\]
Unbiased but high variance.

TD target:
\[
R_{t+1}+\gamma V(S_{t+1})
\]
Biased but low variance.

** Summary MC vs TD

* MC: zero bias, high variance
* TD: some bias, low variance, more efficient

** Random Walk
#+BEGIN_NOTE
Diagram: chain of states A–E with terminal ends and estimated value curves.
#+END_NOTE

** Random Walk: MC vs TD
#+BEGIN_NOTE
Plot of RMS error vs episodes for MC and TD(0) with different α.
#+END_NOTE

** Batch MC and TD
Episodes:
\[
s^k_1,a^k_1,r^k_2,\dots,s^k_{T_k}
\]
Sample episodes repeatedly and apply MC or TD(0).

** AB Example
Two states (A,B); 8 episodes:
\[
A,0,B,0;\quad B,1;\ B,1;\dots;B,0
\]

#+BEGIN_NOTE
Diagram: A→B with reward 0, B→terminal with 1 (75%) or 0 (25%).
#+END_NOTE

** Certainty Equivalence
MC minimises mean-squared error of returns.

TD(0) finds maximum-likelihood Markov model:
\[
\hat P^a_{s,s'}=\frac{1}{N(s,a)}\sum 1(s_t=s,a_t=a,s_{t+1}=s')
\]
\[
\hat R^a_s=\frac{1}{N(s,a)}\sum 1(s_t=s,a_t=a)r_t
\]

In AB example:

* MC: (V(A)=0)
* TD: (V(A)=0.75)

** Bootstrapping vs Sampling

* MC: samples, no bootstrap
* DP: bootstrap, no sampling
* TD: both

** Unified View
#+BEGIN_NOTE
Diagram: DP ↔ TD ↔ MC in a 2-D space of sampling vs bootstrapping.
#+END_NOTE

* TD((\lambda))
  ** n-Step Returns
  [
  G^{(n)}*t = R*{t+1} + \gamma R_{t+2} + \dots + \gamma^{n-1}R_{t+n} + \gamma^n V(S_{t+n})
  ]

Update:
\[
V(S_t)\leftarrow V(S_t)+\alpha(G^{(n)}_t-V(S_t))
\]

** Averaging n-Step Returns
Example:
\[
\frac12 G^{(2)}_t + \frac12 G^{(4)}_t
\]

#+BEGIN_NOTE
Diagram: multiple backups combined into one.
#+END_NOTE

** (\lambda)-Return (Forward View)
\[
G^\lambda_t=(1-\lambda)\sum_{n=1}^{\infty}\lambda^{n-1}G^{(n)}_t
\]

Update:
\[
V(S_t)\leftarrow V(S_t)+\alpha(G^\lambda_t-V(S_t))
\]

#+BEGIN_NOTE
Diagram: weighting of n-step returns by ((1-\lambda)\lambda^{n-1}).
#+END_NOTE

** TD((\lambda)) Weighting
#+BEGIN_NOTE
Plot: exponential decay in weights over time, total area = 1.
#+END_NOTE

** Backward View: Eligibility Traces
\[
E_0(s)=0
\]
\[
E_t(s)=\gamma\lambda E_{t-1}(s)+\mathbf{1}(S_t=s)
\]

** Backward TD((\lambda))
\[
\delta_t=R_{t+1}+\gamma V(S_{t+1})-V(S_t)
\]
\[
V(s)\leftarrow V(s)+\alpha\delta_t E_t(s)
\]

#+BEGIN_NOTE
Diagram: credit assignment spreading backwards through time.
#+END_NOTE

** Special Cases

* (\lambda=0): TD(0)
* (\lambda=1): Monte-Carlo (offline)

** Forward–Backward Equivalence
Offline:
\[
\sum_{t=1}^T \alpha\delta_t E_t(s)
==================================

\sum_{t=1}^T \alpha(G^\lambda_t-V(S_t))\mathbf{1}(S_t=s)
\]

** Telescoping for (\lambda=1)
\[
\delta_t+\gamma\delta_{t+1}+\dots+\gamma^{T-1-t}\delta_{T-1}
=G_t-V(S_t)
\]

** Telescoping for General (\lambda)
\[
G^\lambda_t - V(S_t)
= \delta_t + \gamma\lambda\delta_{t+1} + (\gamma\lambda)^2\delta_{t+2} + \dots
\]

** Summary
Offline:
TD(0) = Forward TD(0)
TD((\lambda)) = Forward TD((\lambda))
TD(1) = MC

Online:
Exact equivalence only with Exact Online TD((\lambda)) (Sutton & van Seijen, 2014).

---

If you want, I can now:

* Split this into one-concept-per-slide with `#+BEGIN_NOTE` speaker notes (as you prefer), or
* Add Mermaid sketches for the Random Walk, AB example, and TD backups.
