<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title>Model-Free Reinforcement Learning</title>
<meta name="author" content="James Brusey"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js/dist/reveal.css"/>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js/dist/theme/serif.css" id="theme"/>

<link rel="stylesheet" href="custom.css"/>
<meta name="description" content="J Brusey - ">
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide">
<h1 class="title">Model-Free Reinforcement Learning</h1><h2 class="author">James Brusey</h2>
</section>
<section>
<section id="slide-orgf7fd6f4">
<h2 id="orgf7fd6f4">Introduction</h2>
<div class="outline-text-2" id="text-orgf7fd6f4">
</div>
</section>
<section id="slide-org607617e">
<h3 id="org607617e">Model-Free Reinforcement Learning</h3>
<ul>
<li>Last lecture: planning by dynamic programming

<ul>
<li>Solve a known Markov Decision Process</li>

</ul></li>
<li>This lecture: model-free prediction

<ul>
<li>Estimate the value function of an unknown Markov Decision Process</li>

</ul></li>
<li>Next lecture: model-free control

<ul>
<li>Optimise the value function of an unknown Markov Decision Process</li>

</ul></li>

</ul>
</section>
</section>
<section>
<section id="slide-org18ed47e">
<h2 id="org18ed47e">Monte-Carlo Learning</h2>
<p>
** Monte-Carlo Reinforcement Learning
</p>

<ul>
<li>Learn directly from episodes of experience</li>
<li>Model-free: no knowledge of transitions or rewards</li>
<li>Learn from complete episodes (no bootstrapping)</li>
<li>Value = mean return</li>
<li>Only applicable to episodic tasks (must terminate)</li>

</ul>
</section>
<section id="slide-orge063784">
<h3 id="orge063784">Monte-Carlo Policy Evaluation</h3>
<p>
Goal: learn (v<sub>&pi;</sub>) from episodes generated by policy (&pi;)
</p>

<p>
Episode:
\[
S_1,A_1,R_2,\dots,S_T
\]
</p>

<p>
Return:
\[
G_t = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{T-1} R_T
\]
</p>

<p>
Value function:
\[
v_\pi(s) = \mathbb{E}_\pi[G_t \mid S_t=s]
\]
</p>

<p>
Monte-Carlo uses empirical mean of returns.
</p>
</section>
<section id="slide-orgd7786db">
<h3 id="orgd7786db">First-Visit Monte-Carlo</h3>
<p>
For state (s):
</p>
</section>
</section>
<section>
<section id="slide-orgac4b87c">
<h2 id="orgac4b87c">Let (t) be the first time (s) appears in an episode</h2>
</section>
</section>
<section>
<section id="slide-orgb019354">
<h2 id="orgb019354">(N(s)&larr; N(s)+1)</h2>
</section>
</section>
<section>
<section id="slide-orgb1061c4">
<h2 id="orgb1061c4">(S(s)&larr; S(s)+G<sub>t</sub>)</h2>
</section>
</section>
<section>
<section id="slide-org33d7fe7">
<h2 id="org33d7fe7">(V(s)=S(s)/N(s))</h2>
<p>
By the law of large numbers:
\[
V(s)\to v_\pi(s)\ \text{as }N(s)\to\infty
\]
</p>
</section>
<section id="slide-org354a2e8">
<h3 id="org354a2e8">Every-Visit Monte-Carlo</h3>
<p>
For every visit of (s) at time (t):
</p>
</section>
</section>
<section>
<section id="slide-orgd50d418">
<h2 id="orgd50d418">(N(s)&larr; N(s)+1)</h2>
</section>
</section>
<section>
<section id="slide-orgc40a7ef">
<h2 id="orgc40a7ef">(S(s)&larr; S(s)+G<sub>t</sub>)</h2>
</section>
</section>
<section>
<section id="slide-org0e069dc">
<h2 id="org0e069dc">(V(s)=S(s)/N(s))</h2>
<p>
Again (V(s)&rarr; v<sub>&pi;</sub>(s)).
</p>
</section>
<section id="slide-org32bd914">
<h3 id="org32bd914">Blackjack Example</h3>
</section>
</section>
<section>
<section id="slide-orgc7c91d4">
<h2 id="orgc7c91d4">State:</h2>
<ul>
<li>Player sum (12!-!21)</li>
<li>Dealer showing card (1!-!10)</li>
<li>Usable ace? yes/no</li>

</ul>
</section>
</section>
<section>
<section id="slide-org339e906">
<h2 id="org339e906">Actions:</h2>
<ul>
<li><b>stick</b>: stop and terminate</li>
<li><b>twist</b>: take another card</li>

</ul>
</section>
</section>
<section>
<section id="slide-org7ef0a45">
<h2 id="org7ef0a45">Rewards:</h2>
<ul>
<li>stick: (+1,0,-1) depending on win/draw/lose</li>
<li>twist: (-1) if bust, else (0)</li>

</ul>
</section>
</section>
<section>
<section id="slide-org6111952">
<h2 id="org6111952">Automatic twist if sum &lt; 12</h2>
<div class="NOTE" id="orgc686089">
<p>
Original slide contains photo of a blackjack table.
</p>

</div>
</section>
<section id="slide-org4a7274f">
<h3 id="org4a7274f">Blackjack Value Function</h3>
<p>
Policy: <b>stick if sum ≥ 20, else twist</b>
</p>

<div class="NOTE" id="org5ccb0d7">
<p>
Diagram: 3-D value surfaces after 10k and 500k episodes,
for usable / non-usable ace.
</p>

</div>
</section>
<section id="slide-orge8d1bd6">
<h3 id="orge8d1bd6">Incremental Mean</h3>
<p>
For data (x<sub>1,&hellip;,x</sub><sub>k</sub>)
\[
\mu_k=\frac{1}{k}\sum_{j=1}^k x_j
=\mu_{k-1}+\frac{1}{k}(x_k-\mu_{k-1})
\]
</p>
</section>
<section id="slide-orga8f9c98">
<h3 id="orga8f9c98">Incremental Monte-Carlo</h3>
<p>
For each visit to state (S<sub>t</sub>) with return (G<sub>t</sub>):
\[
N(S_t)\leftarrow N(S_t)+1
\]
\[
V(S_t)\leftarrow V(S_t)+\frac{1}{N(S_t)}(G_t-V(S_t))
\]
</p>

<p>
With constant step-size:
\[
V(S_t)\leftarrow V(S_t)+\alpha(G_t-V(S_t))
\]
</p>
</section>
</section>
<section>
<section id="slide-org3d0bd1c">
<h2 id="org3d0bd1c">Temporal-Difference Learning</h2>
<p>
** Temporal-Difference Learning
</p>

<ul>
<li>Learn directly from experience</li>
<li>Model-free</li>
<li>Learn from incomplete episodes (bootstrapping)</li>
<li>Update a guess towards a guess</li>

</ul>
</section>
<section id="slide-orge598d86">
<h3 id="orge598d86">MC vs TD</h3>
<p>
Monte-Carlo:
\[
V(S_t)\leftarrow V(S_t)+\alpha(G_t-V(S_t))
\]
</p>

<p>
TD(0):
\[
V(S_t)\leftarrow V(S_t)+\alpha\big(R_{t+1}+\gamma V(S_{t+1})-V(S_t)\big)
\]
</p>

<p>
TD target:
\[
R_{t+1}+\gamma V(S_{t+1})
\]
</p>

<p>
TD error:
\[
\delta_t=R_{t+1}+\gamma V(S_{t+1})-V(S_t)
\]
</p>
</section>
<section id="slide-org863d90c">
<h3 id="org863d90c">Driving Home Example</h3>
<div class="NOTE" id="org4acc5e1">
<p>
Table of states: leaving office → reach car → exit highway → behind truck → home street → arrive home,
with predicted and actual travel times.
</p>

</div>
</section>
<section id="slide-org4012292">
<h3 id="org4012292">Driving Home: MC vs TD</h3>
<div class="NOTE" id="orgf99d402">
<p>
Two plots showing how MC and TD(0) change value estimates over time.
</p>

</div>
</section>
<section id="slide-orgb8ba49e">
<h3 id="orgb8ba49e">MC vs TD: Pros/Cons</h3>
</section>
</section>
<section>
<section id="slide-org5571ba0">
<h2 id="org5571ba0">TD can learn before final outcome</h2>
</section>
</section>
<section>
<section id="slide-org0702110">
<h2 id="org0702110">TD learns online</h2>
</section>
</section>
<section>
<section id="slide-org4df15e4">
<h2 id="org4df15e4">TD works in continuing tasks</h2>
</section>
</section>
<section>
<section id="slide-org08d33ac">
<h2 id="org08d33ac">MC needs full episodes</h2>
</section>
</section>
<section>
<section id="slide-org00687cd">
<h2 id="org00687cd">MC only works in episodic tasks</h2>
<div class="outline-text-2" id="text-org00687cd">
</div>
</section>
<section id="slide-org494af8e">
<h3 id="org494af8e">Bias–Variance</h3>
<p>
Return:
\[
G_t=R_{t+1}+\gamma R_{t+2}+\dots+\gamma^{T-1}R_T
\]
Unbiased but high variance.
</p>

<p>
TD target:
\[
R_{t+1}+\gamma V(S_{t+1})
\]
Biased but low variance.
</p>
</section>
<section id="slide-orgec922d5">
<h3 id="orgec922d5">Summary MC vs TD</h3>
</section>
</section>
<section>
<section id="slide-org2f70266">
<h2 id="org2f70266">MC: zero bias, high variance</h2>
</section>
</section>
<section>
<section id="slide-orga628e7c">
<h2 id="orga628e7c">TD: some bias, low variance, more efficient</h2>
<div class="outline-text-2" id="text-orga628e7c">
</div>
</section>
<section id="slide-org7683201">
<h3 id="org7683201">Random Walk</h3>
<div class="NOTE" id="org1767bc2">
<p>
Diagram: chain of states A–E with terminal ends and estimated value curves.
</p>

</div>
</section>
<section id="slide-orgd6a4ba9">
<h3 id="orgd6a4ba9">Random Walk: MC vs TD</h3>
<div class="NOTE" id="org8155070">
<p>
Plot of RMS error vs episodes for MC and TD(0) with different α.
</p>

</div>
</section>
<section id="slide-orgeea27c8">
<h3 id="orgeea27c8">Batch MC and TD</h3>
<p>
Episodes:
\[
s^k_1,a^k_1,r^k_2,\dots,s^k_{T_k}
\]
Sample episodes repeatedly and apply MC or TD(0).
</p>
</section>
<section id="slide-org4085308">
<h3 id="org4085308">AB Example</h3>
<p>
Two states (A,B); 8 episodes:
\[
A,0,B,0;\quad B,1;\ B,1;\dots;B,0
\]
</p>

<div class="NOTE" id="orgc7a2169">
<p>
Diagram: A→B with reward 0, B→terminal with 1 (75%) or 0 (25%).
</p>

</div>
</section>
<section id="slide-org9f9fa58">
<h3 id="org9f9fa58">Certainty Equivalence</h3>
<p>
MC minimises mean-squared error of returns.
</p>

<p>
TD(0) finds maximum-likelihood Markov model:
\[
\hat P^a_{s,s'}=\frac{1}{N(s,a)}\sum 1(s_t=s,a_t=a,s_{t+1}=s')
\]
\[
\hat R^a_s=\frac{1}{N(s,a)}\sum 1(s_t=s,a_t=a)r_t
\]
</p>

<p>
In AB example:
</p>
</section>
</section>
<section>
<section id="slide-orgc1360bc">
<h2 id="orgc1360bc">MC: (V(A)=0)</h2>
</section>
</section>
<section>
<section id="slide-orge40cea2">
<h2 id="orge40cea2">TD: (V(A)=0.75)</h2>
<div class="outline-text-2" id="text-orge40cea2">
</div>
</section>
<section id="slide-org57128f0">
<h3 id="org57128f0">Bootstrapping vs Sampling</h3>
</section>
</section>
<section>
<section id="slide-org2ecac8f">
<h2 id="org2ecac8f">MC: samples, no bootstrap</h2>
</section>
</section>
<section>
<section id="slide-orga75cdf5">
<h2 id="orga75cdf5">DP: bootstrap, no sampling</h2>
</section>
</section>
<section>
<section id="slide-org300de46">
<h2 id="org300de46">TD: both</h2>
<div class="outline-text-2" id="text-org300de46">
</div>
</section>
<section id="slide-org8c3d044">
<h3 id="org8c3d044">Unified View</h3>
<div class="NOTE" id="orgb5114f1">
<p>
Diagram: DP ↔ TD ↔ MC in a 2-D space of sampling vs bootstrapping.
</p>

</div>
</section>
</section>
<section>
<section id="slide-org669c04c">
<h2 id="org669c04c">TD((&lambda;))</h2>
<p>
** n-Step Returns
[
G<sup>(n)</sup>*t = R*{t+1} + &gamma; R<sub>t+2</sub> + &hellip; + &gamma;<sup>n-1</sup>R<sub>t+n</sub> + &gamma;<sup>n</sup> V(S<sub>t+n</sub>)
]
</p>

<p>
Update:
\[
V(S_t)\leftarrow V(S_t)+\alpha(G^{(n)}_t-V(S_t))
\]
</p>
</section>
<section id="slide-org5918755">
<h3 id="org5918755">Averaging n-Step Returns</h3>
<p>
Example:
\[
\frac12 G^{(2)}_t + \frac12 G^{(4)}_t
\]
</p>

<div class="NOTE" id="org71160ff">
<p>
Diagram: multiple backups combined into one.
</p>

</div>
</section>
<section id="slide-org042d470">
<h3 id="org042d470">(&lambda;)-Return (Forward View)</h3>
<p>
\[
G^\lambda_t=(1-\lambda)\sum_{n=1}^{\infty}\lambda^{n-1}G^{(n)}_t
\]
</p>

<p>
Update:
\[
V(S_t)\leftarrow V(S_t)+\alpha(G^\lambda_t-V(S_t))
\]
</p>

<div class="NOTE" id="org5f3ab6b">
<p>
Diagram: weighting of n-step returns by ((1-&lambda;)&lambda;<sup>n-1</sup>).
</p>

</div>
</section>
<section id="slide-org233fb8c">
<h3 id="org233fb8c">TD((&lambda;)) Weighting</h3>
<div class="NOTE" id="orge48be51">
<p>
Plot: exponential decay in weights over time, total area = 1.
</p>

</div>
</section>
<section id="slide-org7804380">
<h3 id="org7804380">Backward View: Eligibility Traces</h3>
<p>
\[
E_0(s)=0
\]
\[
E_t(s)=\gamma\lambda E_{t-1}(s)+\mathbf{1}(S_t=s)
\]
</p>
</section>
<section id="slide-orgf613c81">
<h3 id="orgf613c81">Backward TD((&lambda;))</h3>
<p>
\[
\delta_t=R_{t+1}+\gamma V(S_{t+1})-V(S_t)
\]
\[
V(s)\leftarrow V(s)+\alpha\delta_t E_t(s)
\]
</p>

<div class="NOTE" id="org8ba15f1">
<p>
Diagram: credit assignment spreading backwards through time.
</p>

</div>
</section>
<section id="slide-org430960b">
<h3 id="org430960b">Special Cases</h3>
</section>
</section>
<section>
<section id="slide-orge19970d">
<h2 id="orge19970d">(&lambda;=0): TD(0)</h2>
</section>
</section>
<section>
<section id="slide-org7dba0bf">
<h2 id="org7dba0bf">(&lambda;=1): Monte-Carlo (offline)</h2>
<div class="outline-text-2" id="text-org7dba0bf">
</div>
</section>
<section id="slide-orgf45ef46">
<h3 id="orgf45ef46">Forward–Backward Equivalence</h3>
<p>
Offline:
\[
&sum;<sub>t=1</sub><sup>T</sup> &alpha;&delta;<sub>t</sub> E<sub>t</sub>(s)
<code>================================</code>
</p>

<p>
&sum;<sub>t=1</sub><sup>T</sup> &alpha;(G^&lambda;<sub>t</sub>-V(S<sub>t</sub>))\mathbf{1}(S<sub>t</sub>=s)
\]
</p>
</section>
<section id="slide-org4494dce">
<h3 id="org4494dce">Telescoping for (&lambda;=1)</h3>
<p>
\[
\delta_t+\gamma\delta_{t+1}+\dots+\gamma^{T-1-t}\delta_{T-1}
=G_t-V(S_t)
\]
</p>
</section>
<section id="slide-org33fb2f1">
<h3 id="org33fb2f1">Telescoping for General (&lambda;)</h3>
<p>
\[
G^\lambda_t - V(S_t)
= \delta_t + \gamma\lambda\delta_{t+1} + (\gamma\lambda)^2\delta_{t+2} + \dots
\]
</p>
</section>
<section id="slide-org470f8da">
<h3 id="org470f8da">Summary</h3>
<p>
Offline:
TD(0) = Forward TD(0)
TD((&lambda;)) = Forward TD((&lambda;))
TD(1) = MC
</p>

<p>
Online:
Exact equivalence only with Exact Online TD((&lambda;)) (Sutton &amp; van Seijen, 2014).
</p>

<p>
&mdash;
</p>

<p>
If you want, I can now:
</p>
</section>
</section>
<section>
<section id="slide-org8b19c9c">
<h2 id="org8b19c9c">Split this into one-concept-per-slide with `#+BEGIN<sub>NOTE</sub>` speaker notes (as you prefer), or</h2>
</section>
</section>
<section>
<section id="slide-orgb1981fc">
<h2 id="orgb1981fc">Add Mermaid sketches for the Random Walk, AB example, and TD backups.</h2>
</section>
</section>
</div>
</div>
<script src="https://cdn.jsdelivr.net/npm/reveal.js/dist/reveal.js"></script>
<script src="https://cdn.jsdelivr.net/npm/reveal.js/plugin/markdown/markdown.js"></script>
<script src="https://cdn.jsdelivr.net/npm/reveal.js/plugin/notes/notes.js"></script>
<script src="https://cdn.jsdelivr.net/npm/reveal.js/plugin/search/search.js"></script>
<script src="https://cdn.jsdelivr.net/npm/reveal.js/plugin/zoom/zoom.js"></script>
<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: 'c',
rollingLinks: false,
keyboard: true,
mouseWheel: false,
fragmentInURL: false,
hashOneBasedIndex: false,
pdfSeparateFragments: true,
overview: true,
margin: 0.10,
minScale: 0.10,
maxScale: 2.50,

transition: 'convex',
transitionSpeed: 'default',

// Plugins with reveal.js 4.x
plugins: [ RevealMarkdown, RevealNotes, RevealSearch, RevealZoom ],

// Optional libraries used to extend reveal.js
dependencies: [
]

});
</script>
</body>
</html>
